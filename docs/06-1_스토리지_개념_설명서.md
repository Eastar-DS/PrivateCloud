# 스토리지 개념 설명서

## 우리 프로젝트의 스토리지를 완벽하게 이해하기

> 이 문서는 스토리지 설계서(06_스토리지_설계서.md)를 이해하기 위한 보조 문서입니다.
> 각 개념마다 **쉬운 설명**(비유)과 **자세한 설명**(기술적)을 함께 제공합니다.

---

## 목차

1. [왜 스토리지가 필요한가?](#1-왜-스토리지가-필요한가)
2. [Ceph란 무엇인가?](#2-ceph란-무엇인가)
3. [OSD — 실제 데이터를 저장하는 일꾼](#3-osd--실제-데이터를-저장하는-일꾼)
4. [MON — 클러스터의 관제탑](#4-mon--클러스터의-관제탑)
5. [CRUSH Map — 데이터를 어디에 둘지 결정하는 규칙](#5-crush-map--데이터를-어디에-둘지-결정하는-규칙)
6. [스토리지 유형: 블록과 파일(NFS)](#6-스토리지-유형-블록과-파일nfs)
7. [NFS — 백엔드가 파일을 저장하는 방법](#7-nfs--백엔드가-파일을-저장하는-방법)
8. [Pool — 데이터를 분류하는 서랍장](#8-pool--데이터를-분류하는-서랍장)
9. [스토리지 쿼터 — 부서별 용량 제한](#9-스토리지-쿼터--부서별-용량-제한)
10. [VM 디스크 vs NFS 파일 스토리지](#10-vm-디스크-vs-nfs-파일-스토리지)
11. [복제(Replication)와 EC(Erasure Coding)](#11-복제replication와-ecerasure-coding)
12. [티어링 — Hot/Warm/Cold 데이터 분류](#12-티어링--hotwarmcold-데이터-분류)
13. [우리 프로젝트에서 스토리지가 실제로 쓰이는 흐름](#13-우리-프로젝트에서-스토리지가-실제로-쓰이는-흐름)
14. [에이블스택 엔지니어에게 물어볼 스토리지 질문 정리](#14-에이블스택-엔지니어에게-물어볼-스토리지-질문-정리)

---

## 1. 왜 스토리지가 필요한가?

### 쉬운 설명

우리 프로젝트는 **문서 태깅 시스템**이다. 사용자가 PDF, 이미지, 워드 파일을 업로드하면 AI가 자동으로 태그를 붙여준다. 그러면 이 **파일들을 어딘가에 저장**해야 한다.

일반적인 서버라면 `/home/user/uploads/` 같은 폴더에 저장하겠지만, 우리는 **프라이빗 클라우드**를 구축하고 있다. 서버가 여러 대이고, 서버 한 대가 죽어도 파일이 안전해야 한다.

그래서 **Ceph라는 분산 스토리지**를 쓴다.

### 자세한 설명

우리 프로젝트의 스토리지 요구사항은 크게 4가지이다:

| 저장 대상 | 설명 | 예시 |
|----------|------|------|
| **문서 원본** | 사용자가 업로드한 파일 | PDF, docx, hwp, 이미지 등 |
| **태깅 결과** | AI가 분석한 결과 데이터 | 태그 목록, 신뢰도 점수 JSON |
| **AI 모델** | KoBERT 분류 모델 파일 | model.pt (수백 MB~수 GB) |
| **VM 디스크** | 서버들의 OS와 DB 데이터 | MariaDB 데이터 파일, OS 이미지 |

단일 서버 로컬 디스크로는 **HA(고가용성)**, **부서 간 격리**, **용량 확장**을 해결할 수 없다. Ceph는 이 모든 걸 소프트웨어로 해결한다.

---

## 2. Ceph란 무엇인가?

### 쉬운 설명

**Ceph는 여러 대의 컴퓨터 하드디스크를 하나의 거대한 저장소로 만들어주는 소프트웨어**이다.

비유하면:
- 일반 하드디스크 = **개인 USB 메모리** (한 개 잃어버리면 끝)
- Ceph = **Google Drive** (어디서든 접근 가능, 알아서 백업됨, 용량 늘리기 쉬움)

차이점은 Google Drive는 구글 서버에 저장되지만, Ceph는 **우리 서버들**에 직접 설치해서 사용한다는 점이다. 그래서 "프라이빗 클라우드 스토리지"인 것이다.

### 자세한 설명

Ceph는 **SDS(Software-Defined Storage)**이다. 물리 디스크들을 추상화하여 소프트웨어로 관리한다.

**Ceph의 핵심 특징:**
- **단일 장애점 없음**: 서버 1대가 죽어도 데이터 접근 가능
- **자동 복제**: 데이터를 자동으로 3곳에 복사 (replica 3)
- **자동 복구**: 서버가 죽으면 자동으로 다른 곳에 복제본 재생성
- **수평 확장**: 디스크/서버 추가만으로 용량 증설 (무중단)
- **다중 인터페이스**: 하나의 클러스터에서 블록/파일 스토리지 등 다양한 접근 방식 제공

**ABLESTACK에서의 Ceph**: ABLESTACK HCI 플랫폼은 내부적으로 Ceph를 사용한다. 물리 호스트 3대(최소) 위에 Ceph 클러스터가 올라가고, 그 위에서 VM들이 실행된다.

```
물리 호스트 3대 (ABLESTACK)
├── Ceph 클러스터 (분산 스토리지)
│   ├── OSD 데몬들 (실제 디스크)
│   ├── MON 데몬들 (클러스터 관리)
│   └── MDS 데몬들 (CephFS 메타데이터 관리)
│
├── KVM 하이퍼바이저 (가상화)
│   ├── Web VM ×2
│   ├── App VM ×2
│   ├── AI VM ×1
│   ├── DB VM ×2
│   └── ...
│
└── Mold (관리 플랫폼 = CloudStack)
```

---

## 3. OSD — 실제 데이터를 저장하는 일꾼

### 쉬운 설명

**OSD(Object Storage Daemon)는 실제로 데이터를 디스크에 읽고 쓰는 프로그램**이다.

비유: **창고의 작업자**
- 물리 디스크 1개 = 창고 하나
- OSD 1개 = 그 창고를 관리하는 직원 1명
- "이 파일 저장해줘" 하면 OSD가 디스크에 기록
- "이 파일 꺼내줘" 하면 OSD가 디스크에서 읽어서 전달

우리 프로젝트에서는:
- 물리 호스트 3대 × 호스트당 NVMe SSD 4개 = **Hot OSD 12개** (빠른 저장소)
- 물리 호스트 3대 × 호스트당 HDD 4개 = **Cold OSD 12개** (저렴한 저장소)

### 자세한 설명

OSD는 Ceph에서 **실제 I/O를 처리하는 데몬 프로세스**이다.

| 항목 | 설명 |
|------|------|
| **1 OSD = 1 물리 디스크** | 디스크마다 OSD 데몬이 하나씩 실행됨 |
| **BlueStore** | OSD의 기본 스토리지 백엔드 (직접 디스크 관리, 파일시스템 불필요) |
| **자동 복구** | OSD가 죽으면 다른 OSD들이 자동으로 데이터를 재복제 |
| **하트비트** | OSD끼리 주기적으로 "살아있니?" 확인 |

```
물리 호스트 1 (NVMe SSD)        물리 호스트 2 (NVMe SSD)
┌───────────────────────┐      ┌───────────────────────┐
│ OSD.0  OSD.1          │      │ OSD.4  OSD.5          │
│ OSD.2  OSD.3          │      │ OSD.6  OSD.7          │
└───────────────────────┘      └───────────────────────┘

→ 파일 A를 저장하면, OSD.0, OSD.5, OSD.9에 3개 복사본 생성
  (서로 다른 호스트에 분산 = 호스트 1대 죽어도 안전)
```

**우리 설계:**
- Hot Tier (NVMe SSD): OSD 12개 → 운영 데이터, DB, 최근 문서
- Cold Tier (HDD): OSD 12개 → 백업, 아카이브

---

## 4. MON — 클러스터의 관제탑

### 쉬운 설명

**MON(Monitor)은 Ceph 클러스터의 "관제탑"**이다.

비유: **비행기 관제탑**
- OSD가 활주로의 비행기(실제 일하는 놈)라면
- MON은 관제탑(누가 어디에 있는지, 다 살아있는지 감시)
- MON이 없으면 OSD들이 서로 어디에 뭐가 있는지 모름

핵심: MON은 **항상 홀수 개(3개)**를 둔다. 왜? **투표**를 하기 때문이다.
- 3개 중 2개가 "OK" 하면 통과 (과반수 = 쿼럼)
- 1개가 죽어도 나머지 2개가 운영 가능
- 2개가 죽으면? 과반수 못 채워서 클러스터 정지

### 자세한 설명

MON이 관리하는 것:

| 관리 대상 | 설명 |
|----------|------|
| **Cluster Map** | 클러스터 전체 지도 (OSD 목록, 상태, 위치) |
| **CRUSH Map** | 데이터 배치 규칙 (어떤 데이터를 어떤 OSD에 저장할지) |
| **OSD Map** | 각 OSD의 상태 (up/down, in/out) |
| **PG Map** | Placement Group 상태 |
| **Auth** | 클라이언트/데몬 인증 정보 |

**쿼럼(Quorum)**: MON 3개 중 과반수(2개 이상)가 합의해야 클러스터 상태를 변경할 수 있다. 이는 **Split Brain**(서로 다른 버전의 진실이 공존하는 상황)을 방지한다.

```
MON.1 ←──── 하트비트 ────→ MON.2
  │                           │
  └───── 하트비트 ────→ MON.3 ─┘

"OSD.5가 죽었다!"
→ MON.1: "동의" / MON.2: "동의" / MON.3: "동의"
→ 과반수 합의 → OSD.5를 dead로 표시 → 자동 복구 시작
```

---

## 5. CRUSH Map — 데이터를 어디에 둘지 결정하는 규칙

### 쉬운 설명

파일을 저장할 때 "3개 복사본을 만든다"고 했다. 그런데 **어느 OSD에 저장할지**는 누가 정할까?

일반적인 방법: 테이블에 기록 → "파일A는 OSD.1, OSD.5, OSD.9에 있다" (테이블이 거대해짐)

Ceph의 방법: **수학 공식으로 계산** → 파일 이름만 알면 어디에 저장되어 있는지 바로 계산 가능

이 수학 공식이 **CRUSH 알고리즘**이고, 거기에 적용되는 규칙이 **CRUSH Map**이다.

CRUSH Map의 핵심 규칙: **"같은 파일의 복사본은 반드시 다른 호스트에 저장하라"**
- 이래야 호스트 1대가 통째로 죽어도 데이터가 안전하다.

### 자세한 설명

**CRUSH = Controlled Replication Under Scalable Hashing**

기존 분산 스토리지의 문제: 메타데이터 서버가 "파일 A → OSD.1, OSD.5" 같은 매핑 테이블을 관리 → 메타데이터 서버가 병목/장애점이 됨

CRUSH의 해결: 매핑 테이블 없이 **알고리즘으로 즉석 계산**
```
CRUSH(파일명, CRUSH Map, 복제 규칙) → [OSD.0, OSD.5, OSD.9]
```

우리 프로젝트의 CRUSH 규칙:
```
# 장애 도메인 = host (호스트 단위 분산)
step chooseleaf firstn 0 type host

# 의미: 복사본 3개를 만들 때, 각각 다른 호스트의 OSD를 선택하라
```

NVMe SSD 전용 규칙도 있다:
```
# NVMe 디바이스 클래스만 사용
step take nvme-class

# 의미: 문서 저장소는 NVMe SSD에만 저장 (성능 보장)
```

---

## 6. 스토리지 유형: 블록과 파일(NFS)

### 쉬운 설명

Ceph는 **하나의 클러스터**인데, 우리 프로젝트에서는 데이터를 꺼내 쓰는 방법을 2가지 사용한다.

| 유형 | 비유 | 우리 프로젝트에서 |
|------|------|-----------------|
| **블록(RBD)** | USB 외장 하드를 꽂은 것 | VM의 C드라이브, MariaDB 데이터 |
| **파일(CephFS/NFS)** | 네트워크 공유 폴더 | **문서 원본, 태깅 결과, AI 모델** |

### 자세한 설명

#### 6.1 블록 스토리지 (RBD = RADOS Block Device)

```
VM이 보는 관점:
┌──────────────┐
│  MariaDB VM  │
│  /dev/vda    │ ← 이 디스크가 사실은 Ceph RBD 이미지
│  (100GB)     │
└──────────────┘

실제 데이터는 Ceph OSD들에 분산 저장:
OSD.0: [vda 조각1] [vda 조각4]
OSD.5: [vda 조각2] [vda 조각5]
OSD.9: [vda 조각3] [vda 조각6]
```

- VM 입장에서는 그냥 하드디스크 하나가 꽂힌 것처럼 보임
- 실제로는 Ceph가 데이터를 분산 저장/복제
- **우리 프로젝트**: VM 8대의 OS 디스크, MariaDB 데이터 볼륨

#### 6.2 파일 스토리지 (CephFS + NFS)

- 일반 파일시스템처럼 디렉토리/파일 구조 사용
- 여러 VM이 동시에 NFS 마운트 가능
- MDS(Metadata Server)가 디렉토리 구조 관리
- **우리 프로젝트**: **문서 원본, OCR 결과, AI 모델 파일 저장에 사용**

```
백엔드 코드에서:
Path path = Paths.get("/mnt/documents", tenantId, "2026/02/10", documentId + ".pdf");
Files.write(path, fileBytes);
                                        ↑ NFS 마운트 포인트의 디렉토리 경로

→ NFS를 통해 CephFS에 저장 → OSD들에 분산 저장
```

- NFS 마운트를 통해 파일을 저장/조회
- 표준 디렉토리/파일 구조 사용
- JDK 표준 `java.nio.file` API로 접근 가능
- **우리 프로젝트**: 문서 원본, OCR 결과, AI 모델 파일 저장

---

## 7. NFS — 백엔드가 파일을 저장하는 방법

### 쉬운 설명

NFS(Network File System)는 **네트워크를 통해 원격 디스크를 내 컴퓨터의 폴더처럼 사용하는 기술**이다. 30년 이상 사용된 검증된 표준이다.

CephFS는 Ceph 클러스터 위에 만들어진 분산 파일시스템인데, 이것을 NFS로 내보내면(export) VM에서 **일반 디렉토리처럼 마운트하여 사용**할 수 있다.

```
비유:
로컬 디스크   = 내 책상 서랍 (나만 접근 가능)
NFS 마운트    = 공용 파일 캐비닛 (여러 사람이 같은 서류함 사용)
```

백엔드 개발자 입장에서는 **로컬 파일을 읽고 쓰는 것과 코드가 100% 동일**하다. 별도의 SDK가 필요 없다:
- 로컬 파일: `Files.write(Paths.get("/home/user/file.pdf"), bytes)`
- NFS 파일: `Files.write(Paths.get("/mnt/documents/tenant/file.pdf"), bytes)`

### 자세한 설명

#### CephFS + NFS란?

CephFS는 Ceph 클러스터 위에 POSIX 호환 파일시스템을 제공하며, MDS(Metadata Server)가 디렉토리 구조를 관리한다. 이를 NFS(포트 2049)로 내보내면 VM에서 표준 마운트로 접근할 수 있다.

```
┌─────────────────────────────────────────────────┐
│  Spring Boot (App 서버, VLAN 20)                  │
│  └── java.nio.file (Files, Paths)                │
│       │                                          │
│       │ 파일 I/O (read/write)                    │
│       ▼                                          │
│  ┌─────────────────────────────────────────┐    │
│  │  NFS 마운트 포인트: /mnt/documents       │    │
│  │  (VLAN 50, 포트 2049)                    │    │
│  │  - 마운트 옵션: soft,timeo=30,retrans=3  │    │
│  │  - 표준 POSIX 파일 I/O                   │    │
│  └─────────────────┬───────────────────────┘    │
│                     │                            │
│                     ▼                            │
│  ┌─────────────────────────────────────────┐    │
│  │  CephFS + MDS (메타데이터 서버)          │    │
│  │  → Ceph RADOS (OSD 클러스터)            │    │
│  │  데이터가 실제로 분산 저장됨            │    │
│  └─────────────────────────────────────────┘    │
└─────────────────────────────────────────────────┘
```

#### 우리 프로젝트의 NFS 사용법

**application.yml 설정:**
```yaml
storage:
  type: nfs
  mount-point: /mnt/documents             # NFS 마운트 포인트
  path-pattern: "{tenantId}/{yyyy}/{MM}/{dd}/{documentId}.{ext}"
```

**디렉토리 구조:**
```
/mnt/documents/                            ← NFS 마운트 포인트 (CephFS)
├── dept-a/2026/02/10/7c9e6679.pdf         ← 문서 원본
├── dept-a/2026/02/10/7c9e6679-ocr.txt     ← OCR 추출 텍스트
│
├── dept-b/2026/02/10/...                  ← 부서 B 문서
│
├── dept-c/2026/02/10/...                  ← 부서 C 문서
│
└── models/                                ← AI 모델 파일
```

**핵심 파일 연산 (백엔드에서 사용하는 것들):**

| 연산 | java.nio.file API | 우리 프로젝트에서 |
|------|-------------------|-----------------|
| 파일 업로드 | `Files.write(path, bytes)` | 사용자가 문서 업로드 시 |
| 파일 다운로드 | `Files.readAllBytes(path)` | AI 서버가 문서 읽을 때 |
| 파일 삭제 | `Files.deleteIfExists(path)` | 문서 삭제 시 |
| 텍스트 읽기 | `Files.readString(path)` | txt 파일 내용 읽기 |
| 텍스트 저장 | `Files.writeString(path, text)` | OCR 결과 텍스트 저장 |

#### NFS 마운트 설정

App 서버와 AI 서버 모두 동일한 NFS를 마운트하여 파일에 직접 접근한다:

```bash
# /etc/fstab 설정
10.0.50.x:/cephfs/documents  /mnt/documents  nfs  soft,timeo=30,retrans=3  0  0

# 수동 마운트
mount -t nfs -o soft,timeo=30,retrans=3 10.0.50.x:/cephfs/documents /mnt/documents
```

App 서버와 AI 서버가 같은 NFS를 마운트하므로, 파일 경로만 전달하면 AI 서버가 별도 다운로드 없이 직접 파일을 읽을 수 있다.

---

## 8. Pool — 데이터를 분류하는 서랍장

### 쉬운 설명

**Pool은 Ceph 안에서 데이터를 분류하는 "서랍장"**이다.

비유: 대형 창고에 구역을 나눠놓은 것
```
창고 전체 = Ceph 클러스터

├── 🏢 시스템 구역 (system-images): OS 설치 파일들
├── 🗄️ DB 구역 (shared-db): MariaDB 데이터
├── 🤖 AI 모델 구역 (model-storage): KoBERT 모델 파일
│
├── 📁 부서A 문서 구역 (dept-a-documents): A부서 PDF 등
├── 📁 부서B 문서 구역 (dept-b-documents): B부서 PDF 등
├── 📁 부서C 문서 구역 (dept-c-documents): C부서 PDF 등
│
├── 🏷️ 부서A 결과 구역 (dept-a-results): A부서 태깅 결과
├── 🏷️ 부서B 결과 구역 (dept-b-results): B부서 태깅 결과
├── 🏷️ 부서C 결과 구역 (dept-c-results): C부서 태깅 결과
│
└── 💾 백업 구역 (backup-pool): 전체 백업 데이터
```

**왜 풀을 나누나?**
1. **격리**: 부서A가 부서B 데이터에 접근 불가
2. **쿼터**: 풀마다 최대 용량 설정 가능
3. **성능 설정**: 풀마다 다른 디스크 사용 가능 (NVMe vs HDD)
4. **복제 정책**: 풀마다 다른 복제 방식 적용 가능

### 자세한 설명

Pool은 Ceph에서 **논리적인 데이터 파티션**이다. 각 Pool은 독립적인 설정을 가진다.

| Pool 이름 | 용도 | 복제 | PG 수 | 쿼터 | 디스크 |
|-----------|------|------|-------|------|--------|
| system-images | OS 템플릿, ISO | 3중 복제 | 64 | 200GB | NVMe |
| shared-db | MariaDB 데이터 | 3중 복제 | 128 | 500GB | NVMe |
| model-storage | AI 모델 파일 | 3중 복제 | 64 | 100GB | NVMe |
| dept-X-documents | 부서별 문서 | 3중 복제 | 128 | 100GB | NVMe |
| dept-X-results | 부서별 태깅 결과 | 3중 복제 | 64 | 50GB | NVMe |
| backup-pool | 백업 데이터 | EC 4+2 | 256 | 2TB | HDD |

**PG(Placement Group)란?**

OSD가 수십 개 있을 때, 파일 하나하나를 직접 OSD에 매핑하면 관리가 복잡하다. 그래서 중간 단계로 **PG**를 둔다.

```
파일들 ──→ PG들 ──→ OSD들

파일 A ─┐
파일 B ─┤──→ PG.1 ──→ [OSD.0, OSD.5, OSD.9]
파일 C ─┘

파일 D ─┐
파일 E ─┤──→ PG.2 ──→ [OSD.1, OSD.6, OSD.10]
파일 F ─┘
```

PG 수가 너무 적으면 데이터가 골고루 분산되지 않고, 너무 많으면 관리 오버헤드가 늘어난다. 보통 **Pool 크기에 비례**하여 설정한다.

---

## 9. 스토리지 쿼터 — 부서별 용량 제한

### 쉬운 설명

쿼터는 **"이 부서는 최대 이만큼만 저장할 수 있다"** 라는 제한이다.

```
부서 A: 문서 100GB + 태깅결과 50GB + 임시 10GB = 총 160GB
부서 B: 문서 100GB + 태깅결과 50GB + 임시 10GB = 총 160GB
부서 C: 문서 100GB + 태깅결과 50GB + 임시 10GB = 총 160GB
```

왜 필요한가?
- 부서 A가 파일을 미친듯이 올려서 전체 스토리지를 다 써버리면 부서 B, C가 못 쓰게 됨
- 쿼터가 있으면 "100GB 다 쓰셨습니다. 오래된 파일 정리해주세요" 같은 제한 가능

### 자세한 설명

Ceph에서 쿼터는 **Pool 레벨** 또는 **CephFS 디렉토리 레벨**에서 설정한다.

```yaml
# Pool 레벨 쿼터
ceph osd pool set dept-a-documents max_bytes 107374182400  # 100GB

# CephFS 디렉토리 레벨 쿼터
setfattr -n ceph.quota.max_bytes -v 107374182400 /mnt/documents/dept-a  # 100GB
```

추가로 **IOPS 쿼터**도 설정 가능하다. 이는 한 부서의 I/O가 다른 부서의 성능에 영향을 주지 않도록 한다.

---

## 10. VM 디스크 vs NFS 파일 스토리지

### 쉬운 설명

이게 헷갈리기 쉬운 부분인데, **같은 Ceph에서 나오지만 사용 방법이 완전히 다르다.**

| | VM 디스크 (RBD) | NFS 파일 스토리지 (CephFS) |
|--|-----------------|--------------------------|
| **비유** | 컴퓨터에 꽂힌 SSD | 공유 파일 캐비닛 |
| **접근 방식** | OS가 직접 읽고 씀 | NFS 마운트 후 파일 I/O |
| **파일시스템** | ext4, xfs 등 있음 | CephFS (POSIX 호환) |
| **누가 쓰나** | VM (OS, MariaDB) | Spring Boot 코드, AI 서버 |
| **예시** | MariaDB가 `/var/lib/mysql`에 데이터 저장 | 코드에서 `Files.write(path, bytes)` |

```
┌──────────────────────────────────────────────────┐
│                  Ceph 클러스터                      │
│                                                    │
│   RBD (블록)                 CephFS/NFS (파일)     │
│   ┌─────────┐               ┌──────────────┐      │
│   │ VM 디스크│               │ /mnt/documents│      │
│   │         │               │              │      │
│   │ DB VM   │               │ Spring Boot  │      │
│   │ App VM  │               │ → 문서 저장  │      │
│   │ Web VM  │               │ → OCR 결과   │      │
│   │ AI VM   │               │ → 모델 파일  │      │
│   └─────────┘               └──────────────┘      │
│                                                    │
│   → 같은 OSD들에 저장되지만, 접근 방식이 다름       │
└──────────────────────────────────────────────────┘
```

### 자세한 설명

**VM 디스크 (RBD):**
1. Mold(ABLESTACK 관리)에서 VM을 생성하면, Ceph RBD 이미지가 만들어짐
2. 이 이미지가 VM에 `/dev/vda`처럼 연결됨 (가상 디스크)
3. VM 안에서는 그냥 로컬 디스크처럼 사용
4. MariaDB는 이 디스크 위의 `/var/lib/mysql`에 데이터 저장

**NFS 파일 스토리지 (CephFS):**
1. CephFS를 NFS로 내보내기(export) 설정
2. App 서버와 AI 서버에서 `/mnt/documents`로 NFS 마운트
3. `java.nio.file` API로 일반 파일처럼 읽기/쓰기
4. 표준 디렉토리/파일 구조로 저장

**왜 문서 파일을 VM 디스크가 아닌 NFS에 저장하나?**
- VM 디스크에 저장하면 **그 VM에서만 접근 가능** (App 서버가 2대인데 파일 공유 불가)
- NFS에 저장하면 **모든 App 서버와 AI 서버에서 같은 경로로 접근 가능** (로드밸런싱 가능)
- CephFS는 **부서별 디렉토리 분리, 디렉토리 쿼터, 표준 파일 권한**이 자연스러움

---

## 11. 복제(Replication)와 EC(Erasure Coding)

### 쉬운 설명

데이터를 보호하는 방법이 2가지이다:

**복제 (Replication) — 같은 걸 3번 복사**
```
원본: [파일A]
복사1: [파일A]   ← Host 1
복사2: [파일A]   ← Host 2
복사3: [파일A]   ← Host 3

장점: 빠르다 (읽을 때 아무 복사본이나 읽으면 됨)
단점: 용량을 3배 쓴다 (100GB 파일 → 300GB 사용)
```

**EC (Erasure Coding) — 쪼개서 복원 코드 추가**
```
원본: [파일A]
→ 4조각으로 나눔: [A1] [A2] [A3] [A4]
→ 복원코드 2개 추가: [P1] [P2]

총 6개를 다른 OSD에 저장
→ 6개 중 아무 4개만 있으면 원본 복원 가능
→ 2개까지 잃어버려도 OK

장점: 용량 효율 (100GB → 150GB만 사용)
단점: 느리다 (읽을 때 조각을 모아서 조립해야 함)
```

**우리 프로젝트:**
- 운영 데이터 (문서, DB): **복제 3중** (빠른 접근 필요)
- 백업/아카이브: **EC 4+2** (용량 절약 필요)

### 자세한 설명

| 비교 | Replication (size=3) | Erasure Coding (k=4, m=2) |
|------|---------------------|--------------------------|
| 저장 효율 | 33% (1/3) | 67% (4/6) |
| 장애 허용 | 2개 OSD 손실 | 2개 OSD 손실 |
| 읽기 성능 | 빠름 (어떤 복사본이든) | 느림 (조각 조합 필요) |
| 쓰기 성능 | 보통 | 느림 (인코딩 연산) |
| CPU 사용 | 낮음 | 높음 (인코딩/디코딩) |
| 용도 | Hot 데이터, DB | Cold 데이터, 백업 |

---

## 12. 티어링 — Hot/Warm/Cold 데이터 분류

### 쉬운 설명

모든 데이터를 비싼 NVMe SSD에 저장하면 돈이 많이 든다. 자주 쓰는 파일만 빠른 디스크에 두고, 안 쓰는 파일은 느리지만 싼 디스크로 옮기자는 것이 **티어링**이다.

```
Hot (NVMe SSD)  → 지금 쓰는 데이터      → 비싸지만 빠름
    │ 30일 안 쓰면 ↓
Warm (SATA SSD) → 가끔 쓰는 데이터      → 중간
    │ 90일 안 쓰면 ↓
Cold (HDD + EC) → 거의 안 쓰는 데이터    → 싸지만 느림

* 갑자기 다시 자주 쓰이면? → Hot으로 승격!
```

**우리 프로젝트 예시:**
- 오늘 올린 문서 → Hot (NVMe) → 빠르게 태깅 처리
- 3개월 전 문서 → Warm (SSD) → 가끔 검색할 때 약간 느림
- 1년 전 문서 → Cold (HDD) → 규정 때문에 보관만 함

### 자세한 설명

| 조건 | 이동 | 대상 |
|------|------|------|
| 30일 미접근 | Hot → Warm | 문서 파일, 태깅 결과 |
| 90일 미접근 | Warm → Cold (+ zstd 압축) | 아카이브 |
| 하루 10회 이상 접근 | Cold/Warm → Hot | 재활성화 데이터 |
| 1년 경과 | 삭제 또는 외부 이전 | 만료 데이터 |

---

## 13. 우리 프로젝트에서 스토리지가 실제로 쓰이는 흐름

### 문서 업로드 → AI 태깅 전체 흐름에서 스토리지가 관여하는 지점

```
사용자가 PDF 업로드
    │
    ▼
① Spring Boot: 파일을 NFS에 저장 ★ 스토리지 사용
   경로: /mnt/documents/dept-a/2026/02/10/{documentId}.pdf
    │
    ▼
② Spring Boot: MariaDB에 메타데이터 저장 ★ VM 디스크(RBD) 위의 DB
   documents 테이블에 INSERT (status: PENDING)
    │
    ▼
③ RabbitMQ에 태깅 요청 메시지 발행
   메시지에 storagePath(NFS 파일 경로) 포함
    │
    ▼
④ Spring AMQP Consumer가 메시지 소비
    │
    ▼
⑤ Consumer → AI OCR 서버 호출 (:8001)
   파일 경로를 전달 → AI 서버가 같은 NFS 마운트에서 직접 파일 읽기 ★ 스토리지 읽기
    │
    ▼
⑥ OCR 결과 텍스트를 NFS에 저장 ★ 스토리지 쓰기
   경로: /mnt/documents/dept-a/2026/02/10/{documentId}-ocr.txt
    │
    ▼
⑦ Consumer → AI 태깅 서버 호출 (:8000)
   텍스트 전달 → 태그 목록 반환
    │
    ▼
⑧ 태그를 MariaDB에 저장 ★ VM 디스크(RBD) 위의 DB
   tags 테이블에 INSERT
    │
    ▼
⑨ 문서 상태 COMPLETED로 업데이트
```

**네트워크 경로:**
```
App 서버 (VLAN 20) ──── VLAN 50 ────→ CephFS/NFS (/mnt/documents)
                   ──── VLAN 40 ────→ MariaDB (RBD 볼륨)
                   ──── VLAN 30 ────→ AI 서버 (같은 NFS 마운트)
```

---

## 14. 에이블스택 엔지니어에게 물어볼 스토리지 질문 정리

### 기본 개념 확인

1. **ABLESTACK에서 Ceph 클러스터는 자동으로 구성되는 건가요, 아니면 별도 설정이 필요한가요?**
   - 이유: HCI이므로 기본 Ceph가 내장인지, 우리가 추가 설정해야 하는지 확인

2. **OSD는 물리 디스크당 하나가 자동 생성되나요? Hot(NVMe)/Cold(HDD) 분리는 어떻게 하나요?**
   - 이유: CRUSH 규칙에서 device class(nvme, hdd) 분리가 자동인지 확인

3. **MON 3개가 기본 구성인가요? 별도로 MON 데몬을 배포해야 하나요?**

### CephFS / NFS 관련 (백엔드 개발에 직접 영향)

4. **CephFS와 MDS는 ABLESTACK에서 기본으로 활성화되어 있나요? 아니면 별도 설치가 필요한가요?**
   - 이유: CephFS가 없으면 NFS 내보내기를 못 함 → 백엔드 파일 저장 불가

5. **CephFS를 NFS로 내보내기(export) 설정은 어떻게 하나요?**
   - 이유: App 서버와 AI 서버에서 NFS 마운트를 해야 함

6. **NFS 서버 주소와 포트(2049)는 어떤 형식인가요? 별도 NFS-Ganesha를 사용하나요?**
   - 이유: VM의 /etc/fstab에 마운트 설정을 해야 함

7. **부서별 디렉토리 생성과 권한 설정은 어떻게 하나요?**
   - 이유: `dept-a`, `dept-b`, `dept-c` 디렉토리를 분리하고 접근 권한을 설정해야 함

8. **CephFS 디렉토리별 용량 쿼터 설정은 어떻게 하나요?**
   - 이유: 부서별 100GB 제한 설정 필요

### Pool 관련

9. **Pool 생성은 Mold에서 하나요, CLI(`ceph osd pool create`)로 하나요?**
   - 이유: 부서별 Pool 분리 필요

10. **Pool과 CephFS 디렉토리의 관계는 어떻게 되나요? 디렉토리가 특정 Pool에 매핑되나요?**
    - 이유: 설계서에서 Pool도 분리하고 디렉토리도 분리하는데, 이 둘의 관계가 명확해야 함

11. **PG 수는 자동으로 설정되나요, 수동으로 해야 하나요?**

### VM 디스크 관련

12. **VM 생성 시 디스크 크기는 어떻게 지정하나요?**
    - 이유: MariaDB VM은 100GB, App VM은 50GB 등 용도별 크기가 다름

13. **VM 디스크 확장이 가능한가요? (온라인 리사이즈)**

### 네트워크 관련

14. **스토리지 네트워크(VLAN 50)는 별도로 구성해야 하나요?**
    - 이유: App 서버 → CephFS/NFS 통신 경로

15. **MTU 9000(Jumbo Frame)은 자동 설정인가요, 수동인가요?**

### 실무 우선순위

**반드시 확인 (백엔드 개발 시작 전)**
- 질문 4~8 (CephFS/NFS 관련): 이것이 안 되면 백엔드에서 파일 저장 자체가 불가능

**구축 시 확인**
- 질문 1~3 (기본 구성): 인프라팀이 클러스터 구축할 때
- 질문 9~11 (Pool): 스토리지팀이 Pool 설정할 때

**운영 시 확인**
- 질문 12~15 (VM, 네트워크): VM 생성 및 네트워크 설정 시

---

## 용어 한눈에 보기

| 용어 | 한줄 설명 | 우리 프로젝트에서 |
|------|----------|-----------------|
| **Ceph** | 분산 스토리지 소프트웨어 | ABLESTACK 내장, 모든 스토리지의 기반 |
| **OSD** | 디스크 하나를 관리하는 데몬 | NVMe 12개(Hot) + HDD 12개(Cold) |
| **MON** | 클러스터 상태 관리 (관제탑) | 3개, 과반수 투표(쿼럼) |
| **CRUSH** | 데이터 배치 알고리즘 | 복사본을 다른 호스트에 분산 |
| **RBD** | 블록 스토리지 (가상 디스크) | VM 디스크, MariaDB 볼륨 |
| **CephFS** | 분산 파일 스토리지 (POSIX 호환) | NFS를 통해 문서 파일 저장의 핵심 |
| **NFS** | 네트워크 파일 공유 프로토콜 | App/AI 서버에서 /mnt/documents로 마운트 |
| **MDS** | CephFS 메타데이터 서버 | 디렉토리 구조 관리, 파일 검색 지원 |
| **Pool** | 논리적 데이터 파티션 | 부서별/용도별 분리 |
| **PG** | Pool 안의 데이터 배치 단위 | Pool 크기에 비례하여 설정 |
| **쿼터** | 용량/파일수 제한 | 부서당 문서 100GB, 결과 50GB |
| **Replication** | 데이터를 N번 복사 | 운영 데이터: 3중 복제 |
| **EC** | 쪼개서 패리티 추가 (RAID 5 비슷) | 백업: EC 4+2 |
| **티어링** | Hot/Warm/Cold 자동 이동 | 30일→Warm, 90일→Cold |

---

## 관련 문서

- [06_스토리지_설계서.md](06_스토리지_설계서.md) - 스토리지 설계서 원본
- [07-4_연동_설계서.md](07-4_연동_설계서.md) - NFS 연동 상세 (백엔드 코드)
- [07_백엔드_설계서.md](07_백엔드_설계서.md) - 백엔드 설계서
- [05_네트워크_보안_설계서.md](05_네트워크_보안_설계서.md) - VLAN 50 스토리지망
