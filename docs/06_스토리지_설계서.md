# 스토리지 설계서

**문서 버전**: 1.0
**작성일**: 2026-01-26
**담당 팀**: 스토리지 팀
**작성자**: 스토리지 팀

---

## 1. 설계 개요

### 1.1 목적

본 문서는 멀티테넌트 프라이빗 클라우드 환경의 스토리지 아키텍처를 정의한다. Ceph 기반 분산 스토리지를 통해 테넌트 간 격리, 성능 티어링, 데이터 보호를 구현한다.

### 1.2 설계 목표

| 목표 | 설명 | 측정 지표 |
|------|------|-----------|
| **데이터 안정성** | 데이터 손실 방지 | 복제 팩터 3, 99.999% 내구성 |
| **테넌트 격리** | 테넌트 간 데이터/성능 격리 | 풀 분리, IOPS 쿼터 |
| **비용 효율성** | 데이터 특성별 최적 저장 | Hot/Warm/Cold 티어링 |
| **재해 복구** | 장애 시 신속한 복구 | RPO < 1시간, RTO < 4시간 |
| **확장성** | 용량 증설 용이성 | 무중단 노드 추가 |

### 1.3 적용 범위

```
┌─────────────────────────────────────────────────────────────┐
│                    스토리지 설계 범위                         │
├─────────────────────────────────────────────────────────────┤
│  [In-Scope]                                                 │
│  • Ceph 클러스터 아키텍처                                    │
│  • 스토리지 풀 설계 (블록/파일/오브젝트)                      │
│  • 티어링 정책 수립                                          │
│  • 테넌트별 쿼터 및 격리                                     │
│  • 스냅샷/백업 정책                                          │
│  • DR 복제 전략                                              │
├─────────────────────────────────────────────────────────────┤
│  [Out-of-Scope]                                             │
│  • 물리 디스크 구매/설치                                     │
│  • 외부 백업 솔루션 연동                                     │
│  • 테이프 아카이브                                           │
└─────────────────────────────────────────────────────────────┘
```

---

## 2. 분산 스토리지 아키텍처

### 2.1 Ceph 클러스터 개요

ABLESTACK은 Ceph 기반 SDS(Software Defined Storage)를 제공한다. 분산 아키텍처를 통해 단일 장애점 없이 고가용성을 보장한다.

```
┌─────────────────────────────────────────────────────────────────┐
│                     Ceph 클러스터 아키텍처                        │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│    ┌─────────┐    ┌─────────┐    ┌─────────┐                   │
│    │  MON 1  │    │  MON 2  │    │  MON 3  │   ← 모니터 (쿼럼)  │
│    └────┬────┘    └────┬────┘    └────┬────┘                   │
│         │              │              │                         │
│         └──────────────┼──────────────┘                         │
│                        │                                        │
│              ┌─────────┴─────────┐                              │
│              │    CRUSH Map      │   ← 데이터 배치 규칙          │
│              └─────────┬─────────┘                              │
│                        │                                        │
│    ┌───────────────────┼───────────────────┐                   │
│    │                   │                   │                   │
│    ▼                   ▼                   ▼                   │
│ ┌──────┐           ┌──────┐           ┌──────┐                 │
│ │ OSD  │           │ OSD  │           │ OSD  │   ← 스토리지    │
│ │ 1-4  │           │ 5-8  │           │ 9-12 │     데몬        │
│ └──────┘           └──────┘           └──────┘                 │
│  Host 1             Host 2             Host 3                  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 2.2 데모 환경 클러스터 구성

| 구성요소 | 수량 | 역할 | 비고 |
|----------|------|------|------|
| **MON** | 3 | 클러스터 상태 관리 | 쿼럼 유지 (과반수) |
| **MGR** | 2 | 관리/모니터링 | Active-Standby |
| **OSD** | 6+ | 데이터 저장 | 호스트당 2개 이상 |
| **MDS** | 2 | CephFS 메타데이터 | 파일 스토리지용 |

### 2.3 복제 정책

#### 복제 팩터 설정

```yaml
# 풀별 복제 설정
pools:
  # 운영 데이터 - 3중 복제
  production:
    size: 3           # 복제본 수
    min_size: 2       # 최소 복제본 (쓰기 허용 기준)

  # 백업/아카이브 - Erasure Coding
  archive:
    profile: ec-4-2   # 4 데이터 + 2 패리티
    # 저장 효율: 66% (vs 복제 33%)
```

#### CRUSH 규칙 (장애 도메인)

```
# 데이터 분산 규칙
rule replicated_rule {
    id 0
    type replicated
    min_size 1
    max_size 10
    step take default
    step chooseleaf firstn 0 type host  # 호스트 단위 분산
    step emit
}
```

**설계 근거**:
- 복제본을 서로 다른 호스트에 배치하여 단일 호스트 장애 시에도 데이터 가용성 보장
- 데모 환경에서는 호스트 단위 분산, 운영 환경에서는 랙 단위 분산 권장

---

## 3. 스토리지 풀 설계

### 3.1 풀 구조

```
┌─────────────────────────────────────────────────────────────────┐
│                        스토리지 풀 구조                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │                     공용 풀 (Shared)                      │  │
│  ├──────────────────────────────────────────────────────────┤  │
│  │  • system-images: OS 템플릿, ISO                         │  │
│  │  • shared-data: 공용 데이터셋                             │  │
│  └──────────────────────────────────────────────────────────┘  │
│                                                                 │
│  ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐  │
│  │  Tenant-A Pool  │ │  Tenant-B Pool  │ │  Tenant-C Pool  │  │
│  ├─────────────────┤ ├─────────────────┤ ├─────────────────┤  │
│  │ • vm-disks      │ │ • vm-disks      │ │ • vm-disks      │  │
│  │ • data-volumes  │ │ • data-volumes  │ │ • data-volumes  │  │
│  │ • snapshots     │ │ • snapshots     │ │ • snapshots     │  │
│  └─────────────────┘ └─────────────────┘ └─────────────────┘  │
│                                                                 │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │                     백업 풀 (Backup)                      │  │
│  ├──────────────────────────────────────────────────────────┤  │
│  │  • backup-daily: 일일 백업                                │  │
│  │  • backup-weekly: 주간 백업                               │  │
│  │  • archive: 장기 보관 (EC 적용)                           │  │
│  └──────────────────────────────────────────────────────────┘  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 3.2 풀별 상세 설정

| 풀 이름 | 용도 | 복제 | PG 수 | 쿼터 |
|---------|------|------|-------|------|
| **system-images** | OS 템플릿, ISO | 3 | 64 | 200GB |
| **shared-data** | 공용 데이터셋 | 3 | 64 | 500GB |
| **tenant-a-vms** | Tenant A VM 디스크 | 3 | 128 | 1TB |
| **tenant-b-vms** | Tenant B VM 디스크 | 3 | 128 | 1TB |
| **tenant-c-vms** | Tenant C VM 디스크 | 3 | 128 | 1TB |
| **backup-pool** | 백업 데이터 | EC 4+2 | 256 | 3TB |

### 3.3 스토리지 유형

#### 3.3.1 블록 스토리지 (RBD)

VM 디스크, 데이터 볼륨에 사용

```yaml
# RBD 이미지 생성 예시
rbd:
  pool: tenant-a-vms
  image: web-server-01-root
  size: 50GB
  features:
    - layering      # 스냅샷 지원
    - exclusive-lock
    - object-map    # 성능 최적화
    - fast-diff     # 스냅샷 비교
```

#### 3.3.2 파일 스토리지 (CephFS)

공유 파일 시스템, NFS 대체

```yaml
# CephFS 설정
cephfs:
  name: shared-fs
  metadata_pool: cephfs-metadata
  data_pool: cephfs-data

  # 디렉토리별 쿼터
  quotas:
    /tenant-a: 500GB
    /tenant-b: 500GB
    /shared: 200GB
```

#### 3.3.3 오브젝트 스토리지 (RGW)

백업, 아카이브, 비정형 데이터

```yaml
# Rados Gateway 설정
rgw:
  zone: default
  buckets:
    - name: tenant-a-backup
      quota: 1TB
      lifecycle:
        - transition_days: 30
          storage_class: COLD
```

---

## 4. 티어링 정책

### 4.1 스토리지 티어 정의

```
┌─────────────────────────────────────────────────────────────────┐
│                      스토리지 티어 계층                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  HOT TIER (NVMe SSD)                                    │   │
│  │  ─────────────────────────────────────────────────────  │   │
│  │  • 용도: 운영 VM 디스크, 활성 데이터베이스               │   │
│  │  • 성능: IOPS 10,000+, 지연시간 < 1ms                   │   │
│  │  • 비용: $$$ (고비용)                                    │   │
│  │  • 보관기간: 실시간 접근 필요 데이터                     │   │
│  └─────────────────────────────────────────────────────────┘   │
│                           │                                     │
│                           ▼ 30일 미접근                         │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  WARM TIER (SATA SSD)                                   │   │
│  │  ─────────────────────────────────────────────────────  │   │
│  │  • 용도: 최근 백업, 준활성 데이터                        │   │
│  │  • 성능: IOPS 5,000+, 지연시간 < 5ms                    │   │
│  │  • 비용: $$ (중간)                                       │   │
│  │  • 보관기간: 30일 ~ 90일                                 │   │
│  └─────────────────────────────────────────────────────────┘   │
│                           │                                     │
│                           ▼ 90일 미접근                         │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  COLD TIER (HDD)                                        │   │
│  │  ─────────────────────────────────────────────────────  │   │
│  │  • 용도: 장기 아카이브, 규정 준수 데이터                 │   │
│  │  • 성능: IOPS 500+, 지연시간 < 20ms                     │   │
│  │  • 비용: $ (저비용)                                      │   │
│  │  • 보관기간: 90일 이상                                   │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 4.2 티어링 규칙

| 조건 | 동작 | 대상 |
|------|------|------|
| 30일 미접근 | Hot → Warm 이동 | 백업, 로그 데이터 |
| 90일 미접근 | Warm → Cold 이동 | 아카이브 대상 |
| 빈번한 접근 감지 | Cold/Warm → Hot 승격 | 재활성화 데이터 |
| 1년 경과 | 삭제 또는 외부 이전 | 만료 데이터 |

### 4.3 자동 티어링 설정

```yaml
# 티어링 정책 설정
tiering_policy:
  name: auto-tier
  rules:
    - name: hot-to-warm
      condition:
        last_access_days: 30
        min_size: 1MB
      action:
        move_to: warm-pool

    - name: warm-to-cold
      condition:
        last_access_days: 90
      action:
        move_to: cold-pool
        compression: zstd  # 압축 적용

    - name: promote-hot
      condition:
        access_count_per_day: 10
      action:
        move_to: hot-pool
        priority: high
```

### 4.4 데모 환경 티어링

데모 환경에서는 단순화된 2티어 구성 적용:

```
┌────────────────────────────────────────┐
│  데모 환경 티어 구성 (2-Tier)           │
├────────────────────────────────────────┤
│                                        │
│   ┌─────────────┐   ┌─────────────┐   │
│   │  Primary    │   │  Backup     │   │
│   │  (SSD)      │   │  (HDD/EC)   │   │
│   │             │   │             │   │
│   │  • VM 디스크│   │  • 스냅샷   │   │
│   │  • 활성 DB  │   │  • 백업본   │   │
│   └─────────────┘   └─────────────┘   │
│                                        │
└────────────────────────────────────────┘
```

---

## 5. 테넌트별 스토리지 격리

### 5.1 격리 계층

```
┌─────────────────────────────────────────────────────────────────┐
│                   테넌트 스토리지 격리 모델                       │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Layer 1: 풀(Pool) 분리                                         │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  tenant-a-pool    tenant-b-pool    tenant-c-pool        │   │
│  │  (완전 분리된 데이터 저장 영역)                          │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
│  Layer 2: 네임스페이스 분리 (CephFS)                            │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  /tenant-a/*      /tenant-b/*      /tenant-c/*          │   │
│  │  (디렉토리 수준 접근 제어)                               │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
│  Layer 3: 접근 키 분리                                          │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  ceph-key-a       ceph-key-b       ceph-key-c           │   │
│  │  (테넌트별 인증 키)                                      │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
│  Layer 4: 네트워크 분리 (VLAN)                                  │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  VLAN 60 (Storage)                                       │   │
│  │  + 테넌트별 스토리지 접근 ACL                            │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 5.2 테넌트별 쿼터 설정

```yaml
# 테넌트 스토리지 쿼터
tenant_quotas:
  tenant-a:
    # 용량 쿼터
    capacity:
      total: 1TB
      vm_disks: 500GB
      data_volumes: 400GB
      snapshots: 100GB
    # IOPS 쿼터
    performance:
      read_iops: 5000
      write_iops: 3000
      read_bps: 500MB/s
      write_bps: 300MB/s

  tenant-b:
    capacity:
      total: 1TB
      vm_disks: 600GB
      data_volumes: 300GB
      snapshots: 100GB
    performance:
      read_iops: 5000
      write_iops: 3000

  tenant-c:
    capacity:
      total: 1TB
      vm_disks: 500GB
      data_volumes: 400GB
      snapshots: 100GB
    performance:
      read_iops: 5000
      write_iops: 3000
```

### 5.3 Ceph 인증 설정

```bash
# 테넌트별 Ceph 사용자 생성
# Tenant A
ceph auth get-or-create client.tenant-a \
  mon 'allow r' \
  osd 'allow rwx pool=tenant-a-vms, allow rwx pool=tenant-a-data' \
  -o /etc/ceph/ceph.client.tenant-a.keyring

# Tenant B
ceph auth get-or-create client.tenant-b \
  mon 'allow r' \
  osd 'allow rwx pool=tenant-b-vms, allow rwx pool=tenant-b-data' \
  -o /etc/ceph/ceph.client.tenant-b.keyring

# Tenant C
ceph auth get-or-create client.tenant-c \
  mon 'allow r' \
  osd 'allow rwx pool=tenant-c-vms, allow rwx pool=tenant-c-data' \
  -o /etc/ceph/ceph.client.tenant-c.keyring
```

---

## 6. 스냅샷 및 백업

### 6.1 스냅샷 정책

```
┌─────────────────────────────────────────────────────────────────┐
│                      스냅샷 정책 계층                            │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  실시간 스냅샷 (Application-Consistent)                  │   │
│  │  ─────────────────────────────────────────────────────  │   │
│  │  • 트리거: 주요 작업 전 (업그레이드, 패치)               │   │
│  │  • 보관: 작업 완료 후 24시간                             │   │
│  │  • 용도: 즉시 롤백                                       │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  일일 스냅샷 (Daily)                                     │   │
│  │  ─────────────────────────────────────────────────────  │   │
│  │  • 시간: 매일 02:00 (업무 외 시간)                       │   │
│  │  • 보관: 7일                                             │   │
│  │  • 대상: 모든 운영 VM                                    │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  주간 스냅샷 (Weekly)                                    │   │
│  │  ─────────────────────────────────────────────────────  │   │
│  │  • 시간: 매주 일요일 03:00                               │   │
│  │  • 보관: 4주                                             │   │
│  │  • 대상: 중요 데이터 VM                                  │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  월간 스냅샷 (Monthly)                                   │   │
│  │  ─────────────────────────────────────────────────────  │   │
│  │  • 시간: 매월 1일 04:00                                  │   │
│  │  • 보관: 12개월                                          │   │
│  │  • 대상: 규정 준수 대상 데이터                           │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 6.2 스냅샷 설정 (Terraform)

```hcl
# 스냅샷 정책 정의
resource "snapshot_policy" "daily" {
  name = "daily-snapshot"

  schedule {
    frequency = "daily"
    time      = "02:00"
    timezone  = "Asia/Seoul"
  }

  retention {
    count = 7
  }

  targets = [
    "tenant-a-vms/*",
    "tenant-b-vms/*",
    "tenant-c-vms/*"
  ]
}

resource "snapshot_policy" "weekly" {
  name = "weekly-snapshot"

  schedule {
    frequency  = "weekly"
    day        = "sunday"
    time       = "03:00"
    timezone   = "Asia/Seoul"
  }

  retention {
    count = 4
  }

  targets = [
    "tenant-*-vms/db-*",
    "tenant-*-vms/app-*"
  ]
}
```

### 6.3 백업 전략

#### 백업 유형

| 유형 | 주기 | 대상 | 보관 | 저장 위치 |
|------|------|------|------|-----------|
| **증분 백업** | 매일 | 전체 VM | 7일 | backup-pool |
| **전체 백업** | 주간 | 전체 VM | 4주 | backup-pool |
| **아카이브** | 월간 | 중요 데이터 | 1년 | archive-pool (EC) |

#### 백업 자동화 (Ansible)

```yaml
# backup-playbook.yml
---
- name: Automated Backup
  hosts: ceph-admin
  vars:
    backup_pool: backup-daily
    retention_days: 7

  tasks:
    - name: Create daily backup
      ceph_rbd_snapshot:
        pool: "{{ item }}"
        snapshot_name: "backup-{{ ansible_date_time.date }}"
        all_images: yes
      loop:
        - tenant-a-vms
        - tenant-b-vms
        - tenant-c-vms

    - name: Export to backup pool
      ceph_rbd_export:
        source_pool: "{{ item }}"
        dest_pool: "{{ backup_pool }}"
        compress: yes
      loop:
        - tenant-a-vms
        - tenant-b-vms
        - tenant-c-vms

    - name: Cleanup old backups
      ceph_rbd_cleanup:
        pool: "{{ backup_pool }}"
        older_than_days: "{{ retention_days }}"
```

### 6.4 백업 검증

```yaml
# 백업 무결성 검증
backup_verification:
  schedule: "monthly"

  steps:
    - name: checksum_verification
      action: compare_checksums
      source: production
      target: backup

    - name: restore_test
      action: test_restore
      sample_size: 10%  # 무작위 10% 샘플 복원 테스트
      target: verify-pool

    - name: data_consistency
      action: verify_data
      method: application_level
```

---

## 7. DR(재해복구) 설계

### 7.1 RPO/RTO 정의

```
┌─────────────────────────────────────────────────────────────────┐
│                    RPO/RTO 목표 정의                             │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌───────────────────────────────────────────────────────────┐ │
│  │  서비스 등급별 RPO/RTO                                     │ │
│  ├───────────────────────────────────────────────────────────┤ │
│  │                                                           │ │
│  │   Tier 1 (Critical)     Tier 2 (Important)   Tier 3 (Normal) │
│  │   ──────────────────    ─────────────────    ────────────── │
│  │   RPO: 15분             RPO: 1시간           RPO: 4시간    │ │
│  │   RTO: 1시간            RTO: 4시간           RTO: 24시간   │ │
│  │                                                           │ │
│  │   • 핵심 DB             • 애플리케이션       • 개발 환경   │ │
│  │   • 인증 서버           • 웹 서버            • 테스트 VM   │ │
│  │                                                           │ │
│  └───────────────────────────────────────────────────────────┘ │
│                                                                 │
│  RPO (Recovery Point Objective): 데이터 손실 허용 시점          │
│  RTO (Recovery Time Objective): 서비스 복구 목표 시간           │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 7.2 데이터 복제 전략

#### 동기 복제 (Tier 1)

```
┌─────────────────────────────────────────────────────────────────┐
│                    동기 복제 (Synchronous)                       │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   Primary Site                         Secondary Site           │
│   ┌─────────────┐     동기 복제        ┌─────────────┐         │
│   │             │  ═══════════════>    │             │         │
│   │   Ceph      │     (실시간)         │   Ceph      │         │
│   │  Cluster    │  <═══════════════    │  Cluster    │         │
│   │             │      ACK             │             │         │
│   └─────────────┘                      └─────────────┘         │
│                                                                 │
│   • 쓰기 완료 = 양쪽 확인 후                                    │
│   • RPO = 0 (데이터 손실 없음)                                  │
│   • 지연시간 증가 (네트워크 의존)                               │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

#### 비동기 복제 (Tier 2/3)

```
┌─────────────────────────────────────────────────────────────────┐
│                   비동기 복제 (Asynchronous)                     │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   Primary Site                         Secondary Site           │
│   ┌─────────────┐     주기적 복제      ┌─────────────┐         │
│   │             │  ───────────────>    │             │         │
│   │   Ceph      │     (15분/1시간)     │   Ceph      │         │
│   │  Cluster    │                      │  Cluster    │         │
│   │             │                      │             │         │
│   └─────────────┘                      └─────────────┘         │
│                                                                 │
│   • 쓰기 = 로컬 확인 후 완료                                    │
│   • RPO = 복제 주기 (15분~1시간)                                │
│   • 성능 영향 최소화                                            │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 7.3 RBD Mirroring 설정

```yaml
# DR 복제 설정 (rbd-mirror)
rbd_mirror:
  # Primary → Secondary 복제
  pools:
    - name: tenant-a-vms
      mode: image  # 이미지별 선택 복제
      schedule: "15m"  # 15분 주기

    - name: tenant-b-vms
      mode: image
      schedule: "1h"  # 1시간 주기

    - name: critical-db
      mode: pool  # 전체 풀 복제
      schedule: "5m"  # 5분 주기 (동기에 가깝게)

# 미러링 활성화 명령
# rbd mirror pool enable tenant-a-vms image
# rbd mirror image enable tenant-a-vms/db-server
```

### 7.4 장애 복구 절차

```
┌─────────────────────────────────────────────────────────────────┐
│                     DR 복구 절차 (Runbook)                       │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  1. 장애 감지 및 선언                                           │
│     ├─ 모니터링 알림 확인                                       │
│     ├─ 장애 범위 파악                                           │
│     └─ DR 선언 (의사결정자 승인)                                │
│                                                                 │
│  2. 복제 상태 확인                                              │
│     ├─ rbd mirror pool status <pool>                           │
│     ├─ 마지막 동기화 시점 확인                                  │
│     └─ 데이터 손실 범위 산정                                    │
│                                                                 │
│  3. Failover 실행                                               │
│     ├─ Primary 미러링 비활성화                                  │
│     ├─ Secondary를 Primary로 승격                               │
│     │   rbd mirror image promote <pool>/<image>                │
│     └─ 네트워크/DNS 전환                                        │
│                                                                 │
│  4. 서비스 복구                                                 │
│     ├─ VM 시작 (우선순위 순)                                    │
│     ├─ 애플리케이션 정상화 확인                                 │
│     └─ 모니터링 지표 점검                                       │
│                                                                 │
│  5. Failback (Primary 복구 후)                                  │
│     ├─ Primary 클러스터 복구                                    │
│     ├─ 역방향 복제 설정                                         │
│     └─ 계획된 Failback 실행                                     │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 7.5 데모 환경 DR

데모 환경에서는 단일 클러스터 내 DR 시뮬레이션:

```yaml
# 데모 DR 구성
demo_dr:
  strategy: "same-cluster-snapshot"

  # 스냅샷 기반 복구 테스트
  test_scenario:
    - create_snapshot: "dr-test-{{ timestamp }}"
    - simulate_failure: true
    - restore_from_snapshot: true
    - verify_recovery: true
    - cleanup: true
```

---

## 8. 성능 최적화

### 8.1 성능 튜닝 파라미터

```yaml
# Ceph 성능 최적화 설정
ceph_tuning:
  # OSD 튜닝
  osd:
    osd_memory_target: 4GB
    osd_op_threads: 8
    osd_disk_threads: 4

  # 클라이언트 튜닝
  client:
    rbd_cache: "true"
    rbd_cache_size: 128MB
    rbd_cache_max_dirty: 96MB
    rbd_cache_target_dirty: 64MB
    rbd_cache_writethrough_until_flush: "true"

  # 네트워크 튜닝
  network:
    ms_async_op_threads: 5
    ms_dispatch_throttle_bytes: 100MB
```

### 8.2 워크로드별 최적화

| 워크로드 | 최적화 전략 | 설정 |
|----------|-------------|------|
| **데이터베이스** | 낮은 지연시간 | rbd_cache=false, SSD 풀 |
| **웹 서버** | 읽기 캐싱 | rbd_cache=true, 대용량 캐시 |
| **로그 수집** | 순차 쓰기 | 대용량 블록, EC 풀 |
| **ML 학습** | 높은 처리량 | 대용량 I/O, 병렬 접근 |

### 8.3 모니터링 메트릭

```yaml
# 스토리지 모니터링 (Prometheus)
storage_metrics:
  # 용량 메트릭
  capacity:
    - ceph_cluster_total_bytes
    - ceph_cluster_used_bytes
    - ceph_pool_bytes_used

  # 성능 메트릭
  performance:
    - ceph_osd_op_r_latency
    - ceph_osd_op_w_latency
    - ceph_pool_rd_bytes
    - ceph_pool_wr_bytes
    - ceph_osd_apply_latency_ms

  # 상태 메트릭
  health:
    - ceph_health_status
    - ceph_osd_up
    - ceph_pg_degraded

# 알림 규칙
alerts:
  - name: StorageCapacityWarning
    condition: ceph_cluster_used_bytes / ceph_cluster_total_bytes > 0.8
    severity: warning

  - name: OSDDown
    condition: ceph_osd_up == 0
    severity: critical

  - name: HighLatency
    condition: ceph_osd_op_w_latency > 100ms
    severity: warning
```

### 8.4 성능 벤치마크 기준

```
┌─────────────────────────────────────────────────────────────────┐
│                     성능 기준 (Baseline)                         │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  워크로드          4K Random    128K Sequential    지연시간     │
│  ──────────────────────────────────────────────────────────────│
│  Hot Tier (SSD)    10,000 IOPS   500 MB/s         < 1ms        │
│  Warm Tier         5,000 IOPS    300 MB/s         < 5ms        │
│  Cold Tier (HDD)   500 IOPS      150 MB/s         < 20ms       │
│                                                                 │
│  * fio 도구로 측정, 70% Read / 30% Write 혼합                   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## 9. 산출물 목록

| 산출물 | 설명 | 담당 |
|--------|------|------|
| **스토리지 설계서** | 본 문서 | 스토리지 팀 |
| **Ceph 구성 매뉴얼** | 클러스터 구축 절차 | 스토리지 팀 |
| **백업/복구 절차서** | 백업 및 DR 운영 가이드 | 스토리지 팀 |
| **Terraform 모듈** | storage/ 모듈 | 스토리지 팀 |
| **Ansible Playbook** | 백업 자동화 | 스토리지 팀 |
| **모니터링 대시보드** | Storage 섹션 | 스토리지 팀 |

---

## 10. 체크리스트

### 설계 검토

- [ ] Ceph 클러스터 구성 완료
- [ ] 테넌트별 풀 생성 및 격리 확인
- [ ] 티어링 정책 적용
- [ ] 스냅샷 정책 설정
- [ ] DR 복제 구성

### 구현 검증

- [ ] 스토리지 IOPS 벤치마크 통과
- [ ] 테넌트 간 격리 테스트
- [ ] 스냅샷 생성/복원 테스트
- [ ] 용량 쿼터 적용 확인
- [ ] DR Failover 테스트

---

## 부록

### A. 용어 정의

| 용어 | 설명 |
|------|------|
| **OSD** | Object Storage Daemon, Ceph 데이터 저장 데몬 |
| **MON** | Monitor, Ceph 클러스터 상태 관리 |
| **CRUSH** | Controlled Replication Under Scalable Hashing |
| **PG** | Placement Group, 데이터 배치 단위 |
| **RBD** | RADOS Block Device, Ceph 블록 스토리지 |
| **EC** | Erasure Coding, 공간 효율적 데이터 보호 |
| **RPO** | Recovery Point Objective, 데이터 손실 허용 시점 |
| **RTO** | Recovery Time Objective, 복구 목표 시간 |

### B. 참고 자료

- Ceph Documentation: https://docs.ceph.com
- ABLESTACK 스토리지 가이드
- Red Hat Ceph Storage Best Practices

---

## 관련 문서

- [01_프로젝트_개요서.md](01_프로젝트_개요서.md)
- [04_컴퓨트_설계서.md](04_컴퓨트_설계서.md)
- [05_네트워크_보안_설계서.md](05_네트워크_보안_설계서.md)
- [07_AI플랫폼_인프라_설계서.md](07_AI플랫폼_인프라_설계서.md) - AI 워크로드용 스토리지 설계
