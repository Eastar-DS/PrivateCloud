# 스토리지 설계서

## 범정부 차세대 AI 공동 활용 인프라 구축 프로젝트
### 공공 데이터 주권 확보를 위한 멀티테넌트 프라이빗 클라우드 설계

| 항목 | 내용 |
|------|------|
| 문서명 | 스토리지 설계서 |
| 버전 | 2.0 |
| 작성일 | 2026-01-30 |
| 담당팀 | 스토리지 팀 |

---

**목차**

1. [설계 개요](#1-설계-개요)
2. [분산 스토리지 아키텍처](#2-분산-스토리지-아키텍처)
3. [스토리지 풀 설계](#3-스토리지-풀-설계)
4. [티어링 정책](#4-티어링-정책)
5. [테넌트별 스토리지 격리](#5-테넌트별-스토리지-격리)
6. [AI 워크로드 스토리지](#6-ai-워크로드-스토리지)
7. [스냅샷 및 백업](#7-스냅샷-및-백업)
8. [DR(재해복구) 설계](#8-dr재해복구-설계)
9. [성능 최적화](#9-성능-최적화)
10. [산출물 및 체크리스트](#10-산출물-및-체크리스트)

---

## 1. 설계 개요

### 1.1 목적

본 문서는 멀티테넌트 프라이빗 클라우드 환경의 스토리지 아키텍처를 정의한다. Ceph 기반 분산 스토리지를 통해 테넌트 간 격리, 성능 티어링, 데이터 보호를 구현하며, AI 워크로드를 위한 고성능 스토리지 계층을 제공한다.

### 1.2 설계 목표

| 목표 | 설명 | 측정 지표 |
|------|------|-----------|
| **데이터 안정성** | 데이터 손실 방지 | 복제 팩터 3, 99.999% 내구성 |
| **테넌트 격리** | 테넌트 간 데이터/성능 격리 | 풀 분리, IOPS 쿼터 |
| **AI 워크로드 지원** | 대용량 모델/데이터셋 저장 | 고처리량, 낮은 지연시간 |
| **비용 효율성** | 데이터 특성별 최적 저장 | Hot/Warm/Cold 티어링 |
| **재해 복구** | 장애 시 신속한 복구 | RPO < 1시간, RTO < 4시간 |
| **확장성** | 용량 증설 용이성 | 무중단 노드 추가 |

### 1.3 적용 범위

```
┌─────────────────────────────────────────────────────────────┐
│                    스토리지 설계 범위                         │
├─────────────────────────────────────────────────────────────┤
│  [In-Scope]                                                 │
│  • Ceph 클러스터 아키텍처                                    │
│  • 스토리지 풀 설계 (블록/파일/오브젝트)                      │
│  • 티어링 정책 수립                                          │
│  • 테넌트별 쿼터 및 격리                                     │
│  • AI 워크로드 스토리지 (모델/데이터셋/체크포인트)            │
│  • AI 스토리지망 (VLAN 60) 연계                              │
│  • 스냅샷/백업 정책                                          │
│  • DR 복제 전략                                              │
├─────────────────────────────────────────────────────────────┤
│  [협업 영역]                                                 │
│  • AI 내부망 연계 (네트워크/보안 팀)                          │
│  • GPU 호스트 스토리지 마운트 (컴퓨트/개발 팀)               │
├─────────────────────────────────────────────────────────────┤
│  [Out-of-Scope]                                             │
│  • 물리 디스크 구매/설치                                     │
│  • 외부 백업 솔루션 연동                                     │
│  • 테이프 아카이브                                           │
└─────────────────────────────────────────────────────────────┘
```

---

## 2. 분산 스토리지 아키텍처

### 2.1 Ceph 클러스터 개요

ABLESTACK은 Ceph 기반 SDS(Software Defined Storage)를 제공한다. 분산 아키텍처를 통해 단일 장애점 없이 고가용성을 보장한다.

```
┌─────────────────────────────────────────────────────────────────┐
│                     Ceph 클러스터 아키텍처                        │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│    ┌─────────┐    ┌─────────┐    ┌─────────┐                   │
│    │  MON 1  │    │  MON 2  │    │  MON 3  │   ← 모니터 (쿼럼)  │
│    └────┬────┘    └────┬────┘    └────┬────┘                   │
│         │              │              │                         │
│         └──────────────┼──────────────┘                         │
│                        │                                        │
│              ┌─────────┴─────────┐                              │
│              │    CRUSH Map      │   ← 데이터 배치 규칙          │
│              └─────────┬─────────┘                              │
│                        │                                        │
│    ┌───────────────────┼───────────────────┐                   │
│    │                   │                   │                   │
│    ▼                   ▼                   ▼                   │
│ ┌──────┐           ┌──────┐           ┌──────┐                 │
│ │ OSD  │           │ OSD  │           │ OSD  │   ← 스토리지    │
│ │ 1-4  │           │ 5-8  │           │ 9-12 │     데몬        │
│ └──────┘           └──────┘           └──────┘                 │
│  Host 1             Host 2             Host 3                  │
│ (NVMe SSD)        (NVMe SSD)          (NVMe SSD)               │
│                                                                 │
│ ┌──────┐           ┌──────┐           ┌──────┐                 │
│ │ OSD  │           │ OSD  │           │ OSD  │   ← Cold Tier   │
│ │13-16 │           │17-20 │           │21-24 │     (HDD)       │
│ └──────┘           └──────┘           └──────┘                 │
│  Host 4             Host 5             Host 6                  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 2.2 클러스터 구성

| 구성요소 | 수량 | 역할 | 비고 |
|----------|------|------|------|
| **MON** | 3 | 클러스터 상태 관리 | 쿼럼 유지 (과반수) |
| **MGR** | 2 | 관리/모니터링 | Active-Standby |
| **OSD (Hot)** | 12+ | 운영 데이터 저장 | NVMe SSD, 호스트당 4개 |
| **OSD (Cold)** | 12+ | 백업/아카이브 | HDD, 호스트당 4개 |
| **MDS** | 2 | CephFS 메타데이터 | 파일 스토리지용 |
| **RGW** | 2 | S3 호환 오브젝트 스토리지 | AI 모델/데이터셋용 |

### 2.3 복제 정책

#### 복제 팩터 설정

```yaml
# 풀별 복제 설정
pools:
  # 운영 데이터 - 3중 복제
  production:
    size: 3           # 복제본 수
    min_size: 2       # 최소 복제본 (쓰기 허용 기준)

  # AI 모델 저장소 - 3중 복제 (고가용성)
  ai-models:
    size: 3
    min_size: 2

  # 백업/아카이브 - Erasure Coding
  archive:
    profile: ec-4-2   # 4 데이터 + 2 패리티
    # 저장 효율: 66% (vs 복제 33%)
```

#### CRUSH 규칙 (장애 도메인)

```
# 데이터 분산 규칙
rule replicated_rule {
    id 0
    type replicated
    min_size 1
    max_size 10
    step take default
    step chooseleaf firstn 0 type host  # 호스트 단위 분산
    step emit
}

# AI 스토리지 전용 규칙 (NVMe SSD 우선)
rule ai_storage_rule {
    id 1
    type replicated
    min_size 1
    max_size 10
    step take nvme-class          # NVMe 디바이스 클래스
    step chooseleaf firstn 0 type host
    step emit
}
```

### 2.4 네트워크 구성

```
┌─────────────────────────────────────────────────────────────────┐
│                   스토리지 네트워크 구성                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   ┌─────────────────────────────────────────────────────────┐  │
│   │  공용 스토리지망 (VLAN 40)                               │  │
│   │  - IP: 10.0.40.0/24                                     │  │
│   │  - MTU: 9000 (Jumbo Frame)                              │  │
│   │  - 용도: 테넌트 VM → Ceph 클러스터                       │  │
│   └─────────────────────────────────────────────────────────┘  │
│                                                                 │
│   ┌─────────────────────────────────────────────────────────┐  │
│   │  AI 스토리지망 (VLAN 60)                                 │  │
│   │  - IP: 10.0.60.0/24                                     │  │
│   │  - MTU: 9000 (Jumbo Frame)                              │  │
│   │  - 용도: GPU 클러스터 → AI 전용 스토리지                 │  │
│   │  - 프로토콜: NFS, S3 (MinIO)                            │  │
│   └─────────────────────────────────────────────────────────┘  │
│                                                                 │
│   ┌─────────────────────────────────────────────────────────┐  │
│   │  Ceph 클러스터 네트워크 (Private)                       │  │
│   │  - IP: 10.0.41.0/24 (별도 VLAN)                         │  │
│   │  - MTU: 9000                                             │  │
│   │  - 용도: OSD 간 복제, 복구 트래픽                        │  │
│   └─────────────────────────────────────────────────────────┘  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## 3. 스토리지 풀 설계

### 3.1 풀 구조

```
┌─────────────────────────────────────────────────────────────────┐
│                        스토리지 풀 구조                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │                     공용 풀 (Shared)                      │  │
│  ├──────────────────────────────────────────────────────────┤  │
│  │  • system-images: OS 템플릿, ISO                         │  │
│  │  • shared-data: 공용 데이터셋                             │  │
│  │  • shared-db: 공용 DB 스토리지                            │  │
│  └──────────────────────────────────────────────────────────┘  │
│                                                                 │
│  ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐  │
│  │  Tenant-A Pool  │ │  Tenant-B Pool  │ │  Tenant-C Pool  │  │
│  ├─────────────────┤ ├─────────────────┤ ├─────────────────┤  │
│  │ • vm-disks      │ │ • vm-disks      │ │ • vm-disks      │  │
│  │ • data-volumes  │ │ • data-volumes  │ │ • data-volumes  │  │
│  │ • snapshots     │ │ • snapshots     │ │ • snapshots     │  │
│  └─────────────────┘ └─────────────────┘ └─────────────────┘  │
│                                                                 │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │                  AI 스토리지 풀 (VLAN 60)                 │  │
│  ├──────────────────────────────────────────────────────────┤  │
│  │  • ai-models: 학습된 모델, 모델 레지스트리                │  │
│  │  • ai-datasets: 학습/추론용 데이터셋                      │  │
│  │  • ai-checkpoints: 학습 체크포인트, 아티팩트              │  │
│  │  • ai-cache: 추론 캐시, 임시 데이터                       │  │
│  └──────────────────────────────────────────────────────────┘  │
│                                                                 │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │                     백업 풀 (Backup)                      │  │
│  ├──────────────────────────────────────────────────────────┤  │
│  │  • backup-daily: 일일 백업                                │  │
│  │  • backup-weekly: 주간 백업                               │  │
│  │  • archive: 장기 보관 (EC 적용)                           │  │
│  └──────────────────────────────────────────────────────────┘  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 3.2 풀별 상세 설정

| 풀 이름 | 용도 | 복제 | PG 수 | 쿼터 | 티어 |
|---------|------|------|-------|------|------|
| **system-images** | OS 템플릿, ISO | 3 | 64 | 200GB | Hot |
| **shared-data** | 공용 데이터셋 | 3 | 64 | 500GB | Hot |
| **shared-db** | 공용 DB | 3 | 128 | 500GB | Hot |
| **tenant-a-vms** | Tenant A VM 디스크 | 3 | 128 | 1TB | Hot |
| **tenant-b-vms** | Tenant B VM 디스크 | 3 | 128 | 1TB | Hot |
| **tenant-c-vms** | Tenant C VM 디스크 | 3 | 128 | 1TB | Hot |
| **ai-models** | AI 모델 저장소 | 3 | 256 | 2TB | Hot |
| **ai-datasets** | AI 데이터셋 | 3 | 256 | 5TB | Hot/Warm |
| **ai-checkpoints** | 체크포인트/아티팩트 | 3 | 128 | 1TB | Hot |
| **ai-cache** | 추론 캐시 | 2 | 64 | 500GB | Hot |
| **backup-pool** | 백업 데이터 | EC 4+2 | 256 | 5TB | Cold |

### 3.3 스토리지 유형

#### 3.3.1 블록 스토리지 (RBD)

VM 디스크, 데이터 볼륨에 사용

```yaml
# RBD 이미지 생성 예시
rbd:
  pool: tenant-a-vms
  image: web-server-01-root
  size: 50GB
  features:
    - layering      # 스냅샷 지원
    - exclusive-lock
    - object-map    # 성능 최적화
    - fast-diff     # 스냅샷 비교
```

#### 3.3.2 파일 스토리지 (CephFS)

공유 파일 시스템, NFS 대체

```yaml
# CephFS 설정
cephfs:
  name: shared-fs
  metadata_pool: cephfs-metadata
  data_pool: cephfs-data

  # 디렉토리별 쿼터
  quotas:
    /tenant-a: 500GB
    /tenant-b: 500GB
    /tenant-c: 500GB
    /ai-shared: 2TB
```

#### 3.3.3 오브젝트 스토리지 (RGW)

AI 모델/데이터셋, 백업, 비정형 데이터

```yaml
# Rados Gateway 설정 (S3 호환)
rgw:
  zone: default

  # AI 스토리지 버킷
  buckets:
    - name: ai-model-registry
      quota: 2TB
      versioning: enabled
      lifecycle:
        - transition_days: 90
          storage_class: WARM

    - name: ai-datasets
      quota: 5TB
      versioning: enabled

    - name: ai-artifacts
      quota: 1TB
      lifecycle:
        - transition_days: 30
          storage_class: WARM
        - transition_days: 90
          storage_class: COLD
```

---

## 4. 티어링 정책

### 4.1 스토리지 티어 정의

```
┌─────────────────────────────────────────────────────────────────┐
│                      스토리지 티어 계층                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  HOT TIER (NVMe SSD)                                    │   │
│  │  ─────────────────────────────────────────────────────  │   │
│  │  • 용도: 운영 VM, 활성 DB, AI 학습/추론, 모델 서빙       │   │
│  │  • 성능: IOPS 50,000+, 지연시간 < 0.5ms                 │   │
│  │  • 비용: $$$ (고비용)                                    │   │
│  │  • 대상: 실시간 접근 필요 데이터                         │   │
│  └─────────────────────────────────────────────────────────┘   │
│                           │                                     │
│                           ▼ 30일 미접근                         │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  WARM TIER (SATA SSD)                                   │   │
│  │  ─────────────────────────────────────────────────────  │   │
│  │  • 용도: 최근 백업, 준활성 데이터, 오래된 모델 버전      │   │
│  │  • 성능: IOPS 10,000+, 지연시간 < 2ms                   │   │
│  │  • 비용: $$ (중간)                                       │   │
│  │  • 대상: 30일 ~ 90일 보관                                │   │
│  └─────────────────────────────────────────────────────────┘   │
│                           │                                     │
│                           ▼ 90일 미접근                         │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  COLD TIER (HDD + EC)                                   │   │
│  │  ─────────────────────────────────────────────────────  │   │
│  │  • 용도: 장기 아카이브, 규정 준수 데이터, 학습 로그      │   │
│  │  • 성능: IOPS 1,000+, 지연시간 < 10ms                   │   │
│  │  • 비용: $ (저비용)                                      │   │
│  │  • 대상: 90일 이상 보관                                  │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 4.2 티어링 규칙

| 조건 | 동작 | 대상 |
|------|------|------|
| 30일 미접근 | Hot → Warm 이동 | 백업, 로그 데이터, 오래된 모델 버전 |
| 90일 미접근 | Warm → Cold 이동 | 아카이브 대상 |
| 빈번한 접근 감지 | Cold/Warm → Hot 승격 | 재활성화 데이터 |
| AI 모델 배포 | Warm → Hot 승격 | 모델 서빙용 |
| 1년 경과 | 삭제 또는 외부 이전 | 만료 데이터 |

### 4.3 자동 티어링 설정

```yaml
# 티어링 정책 설정
tiering_policy:
  name: auto-tier
  rules:
    - name: hot-to-warm
      condition:
        last_access_days: 30
        min_size: 1MB
      action:
        move_to: warm-pool

    - name: warm-to-cold
      condition:
        last_access_days: 90
      action:
        move_to: cold-pool
        compression: zstd  # 압축 적용

    - name: promote-hot
      condition:
        access_count_per_day: 10
      action:
        move_to: hot-pool
        priority: high

    - name: ai-model-active
      condition:
        tag: "model-serving"
      action:
        keep_in: hot-pool
        priority: critical
```

---

## 5. 테넌트별 스토리지 격리

### 5.1 격리 계층

```
┌─────────────────────────────────────────────────────────────────┐
│                   테넌트 스토리지 격리 모델                       │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Layer 1: 풀(Pool) 분리                                         │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  tenant-a-pool    tenant-b-pool    tenant-c-pool        │   │
│  │  (완전 분리된 데이터 저장 영역)                          │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
│  Layer 2: 네임스페이스 분리 (CephFS)                            │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  /tenant-a/*      /tenant-b/*      /tenant-c/*          │   │
│  │  (디렉토리 수준 접근 제어)                               │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
│  Layer 3: 접근 키 분리                                          │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  ceph-key-a       ceph-key-b       ceph-key-c           │   │
│  │  (테넌트별 인증 키)                                      │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
│  Layer 4: 네트워크 분리 (VLAN)                                  │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  VLAN 40 (공용 스토리지망)                               │   │
│  │  + 테넌트별 스토리지 접근 ACL                            │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 5.2 테넌트별 쿼터 설정

```yaml
# 테넌트 스토리지 쿼터
tenant_quotas:
  tenant-a:
    # 용량 쿼터
    capacity:
      total: 1.5TB
      vm_disks: 800GB
      data_volumes: 500GB
      snapshots: 200GB
    # IOPS 쿼터
    performance:
      read_iops: 10000
      write_iops: 5000
      read_bps: 1GB/s
      write_bps: 500MB/s
    # AI 스토리지 쿼터 (공용 AI 풀 내)
    ai_quota:
      model_storage: 50GB
      dataset_access: true  # 공용 데이터셋 읽기
      checkpoint_storage: 20GB

  tenant-b:
    capacity:
      total: 1.5TB
      vm_disks: 900GB
      data_volumes: 400GB
      snapshots: 200GB
    performance:
      read_iops: 10000
      write_iops: 5000
    ai_quota:
      model_storage: 100GB
      dataset_access: true
      checkpoint_storage: 50GB

  tenant-c:
    capacity:
      total: 1.5TB
      vm_disks: 800GB
      data_volumes: 500GB
      snapshots: 200GB
    performance:
      read_iops: 10000
      write_iops: 5000
    ai_quota:
      model_storage: 50GB
      dataset_access: true
      checkpoint_storage: 20GB
```

### 5.3 Ceph 인증 설정

```bash
# 테넌트별 Ceph 사용자 생성
# Tenant A
ceph auth get-or-create client.tenant-a \
  mon 'allow r' \
  osd 'allow rwx pool=tenant-a-vms, allow rwx pool=tenant-a-data' \
  mds 'allow rw path=/tenant-a' \
  -o /etc/ceph/ceph.client.tenant-a.keyring

# Tenant B
ceph auth get-or-create client.tenant-b \
  mon 'allow r' \
  osd 'allow rwx pool=tenant-b-vms, allow rwx pool=tenant-b-data' \
  mds 'allow rw path=/tenant-b' \
  -o /etc/ceph/ceph.client.tenant-b.keyring

# AI 서비스 계정 (내부망 전용)
ceph auth get-or-create client.ai-service \
  mon 'allow r' \
  osd 'allow rwx pool=ai-models, allow rwx pool=ai-datasets, allow rwx pool=ai-checkpoints, allow rwx pool=ai-cache' \
  mds 'allow rw path=/ai-shared' \
  -o /etc/ceph/ceph.client.ai-service.keyring
```

---

## 6. AI 워크로드 스토리지

### 6.1 AI 스토리지 아키텍처

```
┌─────────────────────────────────────────────────────────────────┐
│                   AI 스토리지 아키텍처                            │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   ┌─────────────────────────────────────────────────────────┐  │
│   │  AI 내부망 (VLAN 50)                                    │  │
│   │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐     │  │
│   │  │ GPU Host 1  │  │ GPU Host 2  │  │ GPU Host 3  │     │  │
│   │  │ (학습)      │  │ (학습)      │  │ (추론)      │     │  │
│   │  └──────┬──────┘  └──────┬──────┘  └──────┬──────┘     │  │
│   │         │                │                │             │  │
│   └─────────┼────────────────┼────────────────┼─────────────┘  │
│             │                │                │                 │
│             └────────────────┼────────────────┘                 │
│                              │                                  │
│                              │ 10Gbps+ (Jumbo Frame)            │
│                              ▼                                  │
│   ┌─────────────────────────────────────────────────────────┐  │
│   │  AI 스토리지망 (VLAN 60)                                 │  │
│   │                                                         │  │
│   │   ┌───────────────┐   ┌───────────────┐                │  │
│   │   │ Model Registry│   │  Dataset      │                │  │
│   │   │ (S3/MinIO)    │   │  Storage      │                │  │
│   │   │               │   │  (CephFS)     │                │  │
│   │   │ • 모델 버전   │   │ • 학습 데이터 │                │  │
│   │   │ • 메타데이터  │   │ • 검증 데이터 │                │  │
│   │   │ • 서빙 모델   │   │ • 전처리 결과 │                │  │
│   │   └───────────────┘   └───────────────┘                │  │
│   │                                                         │  │
│   │   ┌───────────────┐   ┌───────────────┐                │  │
│   │   │ Checkpoint    │   │  Cache        │                │  │
│   │   │ Storage       │   │  Storage      │                │  │
│   │   │               │   │               │                │  │
│   │   │ • 학습 상태   │   │ • 추론 캐시   │                │  │
│   │   │ • 중간 결과   │   │ • 임시 데이터 │                │  │
│   │   │ • 아티팩트    │   │ • 배치 결과   │                │  │
│   │   └───────────────┘   └───────────────┘                │  │
│   │                                                         │  │
│   └─────────────────────────────────────────────────────────┘  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 6.2 AI 스토리지 유형별 설계

#### 6.2.1 모델 레지스트리 (Model Registry)

```yaml
# 모델 레지스트리 스토리지 설계
model_registry:
  backend: MinIO (S3 호환)
  pool: ai-models

  structure:
    # 버킷 구조
    buckets:
      - name: models
        versioning: enabled
        lifecycle:
          - rule: archive-old-versions
            days: 90
            action: transition-to-warm

      - name: model-artifacts
        # 모델 관련 아티팩트 (config, vocab 등)

  # 모델 저장 형식
  model_formats:
    - pytorch (.pt, .pth)
    - tensorflow (SavedModel)
    - onnx (.onnx)
    - safetensors (.safetensors)

  # 디렉토리 구조
  directory_structure: |
    models/
    ├── text-classifier/
    │   ├── v1.0.0/
    │   │   ├── model.pt
    │   │   ├── config.json
    │   │   └── metadata.json
    │   └── v1.1.0/
    │       └── ...
    └── image-detector/
        └── ...

  # 용량 계획
  capacity:
    initial: 500GB
    max: 2TB
    model_size_limit: 50GB  # 단일 모델 최대 크기
```

#### 6.2.2 데이터셋 스토리지

```yaml
# 데이터셋 스토리지 설계
dataset_storage:
  backend: CephFS + S3
  pools:
    - ai-datasets (primary)
    - ai-datasets-warm (티어링)

  # 데이터셋 유형별 저장
  dataset_types:
    structured:
      format: parquet, csv
      location: cephfs://ai-datasets/structured/

    unstructured:
      format: images, documents
      location: s3://ai-datasets/unstructured/

    preprocessed:
      format: tfrecord, numpy
      location: cephfs://ai-datasets/preprocessed/

  # 디렉토리 구조
  directory_structure: |
    datasets/
    ├── raw/                    # 원본 데이터
    │   ├── tenant-a/           # 테넌트별 분리
    │   ├── tenant-b/
    │   └── shared/             # 공용 데이터셋
    ├── preprocessed/           # 전처리 완료
    │   └── {dataset_id}/
    └── validated/              # 품질 검증 완료
        └── {dataset_id}/

  # 테넌트별 데이터 격리
  tenant_isolation:
    method: directory_acl
    encryption: per-tenant-key

  # 용량 계획
  capacity:
    total: 5TB
    per_tenant: 500GB
    shared_datasets: 2TB
```

#### 6.2.3 체크포인트/아티팩트 스토리지

```yaml
# 체크포인트 스토리지 설계
checkpoint_storage:
  backend: CephFS (고성능)
  pool: ai-checkpoints

  # 체크포인트 정책
  checkpoint_policy:
    # 학습 중 자동 저장
    auto_save:
      interval: 1000  # steps
      keep_last: 5    # 최근 5개 유지

    # 베스트 모델 저장
    best_model:
      metric: validation_loss
      keep_top: 3     # 상위 3개 유지

  # 디렉토리 구조
  directory_structure: |
    checkpoints/
    ├── {job_id}/
    │   ├── checkpoint_1000/
    │   │   ├── model_state.pt
    │   │   ├── optimizer_state.pt
    │   │   └── scheduler_state.pt
    │   ├── checkpoint_2000/
    │   └── best_model/
    └── artifacts/
        ├── tensorboard_logs/
        ├── metrics/
        └── visualizations/

  # 자동 정리 정책
  cleanup:
    - condition: job_completed
      action: archive_to_warm
      delay: 7d
    - condition: older_than_90d
      action: delete

  # 용량 계획
  capacity:
    total: 1TB
    per_job: 100GB
```

#### 6.2.4 추론 캐시 스토리지

```yaml
# 추론 캐시 설계
inference_cache:
  backend: Ceph RBD (고속)
  pool: ai-cache

  # 캐시 유형
  cache_types:
    model_cache:
      # 자주 사용하는 모델 메모리 매핑
      size: 100GB
      eviction: lru

    embedding_cache:
      # 임베딩 벡터 캐시
      size: 200GB
      ttl: 24h

    result_cache:
      # 추론 결과 캐시
      size: 100GB
      ttl: 1h

  # 성능 요구사항
  performance:
    read_latency: < 1ms
    read_iops: 100000+
    throughput: 5GB/s

  # 복제 설정 (낮은 복제, 고성능)
  replication:
    size: 2  # 2중 복제 (성능 우선)
```

### 6.3 AI 스토리지 성능 최적화

```yaml
# AI 워크로드별 스토리지 최적화
ai_storage_optimization:
  # 학습 워크로드
  training:
    read_ahead: 4MB          # 대용량 순차 읽기
    stripe_size: 4MB         # 큰 스트라이프
    num_stripes: 8           # 병렬 읽기
    cache: writeback         # 쓰기 버퍼링

  # 추론 워크로드
  inference:
    read_ahead: 256KB        # 랜덤 읽기 최적화
    cache: writethrough      # 일관성 우선
    prefetch: enabled        # 모델 프리페치

  # 체크포인트 저장
  checkpointing:
    compression: lz4         # 빠른 압축
    async_write: true        # 비동기 쓰기
    direct_io: false         # 버퍼 캐시 활용
```

### 6.4 AI 스토리지 접근 제어

```yaml
# AI 스토리지 RBAC
ai_storage_rbac:
  roles:
    # AI 플랫폼 관리자
    ai-admin:
      permissions:
        - ai-models: rwxd
        - ai-datasets: rwxd
        - ai-checkpoints: rwxd
        - ai-cache: rwxd

    # 학습 작업 (Job)
    training-job:
      permissions:
        - ai-datasets: r
        - ai-checkpoints: rw
        - ai-models: rw

    # 추론 서비스
    inference-service:
      permissions:
        - ai-models: r
        - ai-cache: rw

    # 테넌트 사용자
    tenant-user:
      permissions:
        - ai-models/{tenant_id}: rw
        - ai-datasets/shared: r
        - ai-datasets/{tenant_id}: rw
        - ai-checkpoints/{tenant_id}: rw
```

### 6.5 테넌트별 AI 스토리지 쿼터

| 테넌트 | 모델 저장 | 데이터셋 | 체크포인트 | 캐시 |
|--------|----------|----------|-----------|------|
| 테넌트 A | 50GB | 200GB (공용 읽기) | 20GB | 공유 |
| 테넌트 B | 100GB | 500GB (공용 읽기) | 50GB | 공유 |
| 테넌트 C | 50GB | 200GB (공용 읽기) | 20GB | 공유 |
| **공용** | 2TB | 2TB | 500GB | 500GB |

---

## 7. 스냅샷 및 백업

### 7.1 스냅샷 정책

```
┌─────────────────────────────────────────────────────────────────┐
│                      스냅샷 정책 계층                            │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  실시간 스냅샷 (Application-Consistent)                  │   │
│  │  ─────────────────────────────────────────────────────  │   │
│  │  • 트리거: 주요 작업 전 (업그레이드, 패치, 모델 배포)    │   │
│  │  • 보관: 작업 완료 후 24시간                             │   │
│  │  • 용도: 즉시 롤백                                       │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  일일 스냅샷 (Daily)                                     │   │
│  │  ─────────────────────────────────────────────────────  │   │
│  │  • 시간: 매일 02:00 (업무 외 시간)                       │   │
│  │  • 보관: 7일                                             │   │
│  │  • 대상: 모든 운영 VM, AI 모델 레지스트리               │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  주간 스냅샷 (Weekly)                                    │   │
│  │  ─────────────────────────────────────────────────────  │   │
│  │  • 시간: 매주 일요일 03:00                               │   │
│  │  • 보관: 4주                                             │   │
│  │  • 대상: 중요 데이터 VM, AI 데이터셋                     │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  월간 스냅샷 (Monthly)                                   │   │
│  │  ─────────────────────────────────────────────────────  │   │
│  │  • 시간: 매월 1일 04:00                                  │   │
│  │  • 보관: 12개월                                          │   │
│  │  • 대상: 규정 준수 대상 데이터, 프로덕션 모델            │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 7.2 AI 모델 버전 관리

```yaml
# AI 모델 스냅샷/버전 정책
ai_model_versioning:
  # 자동 버전 관리
  auto_versioning:
    trigger:
      - model_save
      - model_deploy
    format: "v{major}.{minor}.{patch}"

  # 스냅샷 정책
  snapshot:
    before_deploy: true      # 배포 전 스냅샷
    after_training: true     # 학습 완료 후
    retention:
      deployed_versions: 5   # 배포 버전 5개 유지
      all_versions: 90d      # 전체 90일 유지

  # 롤백 정책
  rollback:
    enabled: true
    max_rollback_versions: 3
    auto_rollback_on_error: true
```

### 7.3 백업 전략

| 유형 | 주기 | 대상 | 보관 | 저장 위치 |
|------|------|------|------|-----------|
| **증분 백업** | 매일 | 전체 VM | 7일 | backup-pool |
| **전체 백업** | 주간 | 전체 VM | 4주 | backup-pool |
| **AI 모델 백업** | 배포시 | 프로덕션 모델 | 1년 | archive-pool |
| **데이터셋 백업** | 주간 | 공용 데이터셋 | 90일 | backup-pool |
| **아카이브** | 월간 | 중요 데이터 | 3년 | archive-pool (EC) |

---

## 8. DR(재해복구) 설계

### 8.1 RPO/RTO 정의

```
┌─────────────────────────────────────────────────────────────────┐
│                    RPO/RTO 목표 정의                             │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌───────────────────────────────────────────────────────────┐ │
│  │  서비스 등급별 RPO/RTO                                     │ │
│  ├───────────────────────────────────────────────────────────┤ │
│  │                                                           │ │
│  │  Tier 1 (Critical)       Tier 2 (Important)   Tier 3      │ │
│  │  ──────────────────      ─────────────────    ──────────  │ │
│  │  RPO: 15분               RPO: 1시간           RPO: 4시간  │ │
│  │  RTO: 1시간              RTO: 4시간           RTO: 24시간 │ │
│  │                                                           │ │
│  │  • 핵심 DB               • 애플리케이션       • 개발 환경 │ │
│  │  • 인증 서버             • 웹 서버            • 테스트 VM │ │
│  │  • AI 추론 서비스        • AI 학습 환경       • AI 캐시   │ │
│  │  • 프로덕션 모델         • 데이터셋           • 체크포인트│ │
│  │                                                           │ │
│  └───────────────────────────────────────────────────────────┘ │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 8.2 데이터 복제 전략

#### 동기 복제 (Tier 1)

```
┌─────────────────────────────────────────────────────────────────┐
│                    동기 복제 (Synchronous)                       │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   Primary Site                         Secondary Site           │
│   ┌─────────────┐     동기 복제        ┌─────────────┐         │
│   │             │  ═══════════════>    │             │         │
│   │   Ceph      │     (실시간)         │   Ceph      │         │
│   │  Cluster    │  <═══════════════    │  Cluster    │         │
│   │             │      ACK             │             │         │
│   └─────────────┘                      └─────────────┘         │
│                                                                 │
│   • 쓰기 완료 = 양쪽 확인 후                                    │
│   • RPO = 0 (데이터 손실 없음)                                  │
│   • 대상: 핵심 DB, 프로덕션 AI 모델                             │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

#### 비동기 복제 (Tier 2/3)

```
┌─────────────────────────────────────────────────────────────────┐
│                   비동기 복제 (Asynchronous)                     │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   Primary Site                         Secondary Site           │
│   ┌─────────────┐     주기적 복제      ┌─────────────┐         │
│   │             │  ───────────────>    │             │         │
│   │   Ceph      │     (15분/1시간)     │   Ceph      │         │
│   │  Cluster    │                      │  Cluster    │         │
│   │             │                      │             │         │
│   └─────────────┘                      └─────────────┘         │
│                                                                 │
│   • RPO = 복제 주기 (15분~1시간)                                │
│   • 대상: 일반 VM, AI 데이터셋, 체크포인트                      │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 8.3 RBD Mirroring 설정

```yaml
# DR 복제 설정 (rbd-mirror)
rbd_mirror:
  pools:
    - name: tenant-a-vms
      mode: image
      schedule: "15m"

    - name: ai-models
      mode: pool      # 전체 풀 복제
      schedule: "5m"  # 5분 주기 (중요 데이터)

    - name: ai-datasets
      mode: image
      schedule: "1h"

    - name: critical-db
      mode: pool
      schedule: "5m"
```

---

## 9. 성능 최적화

### 9.1 워크로드별 최적화

| 워크로드 | 최적화 전략 | 설정 |
|----------|-------------|------|
| **데이터베이스** | 낮은 지연시간 | rbd_cache=false, NVMe 풀 |
| **웹 서버** | 읽기 캐싱 | rbd_cache=true, 대용량 캐시 |
| **AI 학습** | 높은 처리량 | 대용량 I/O, 병렬 접근, 스트라이핑 |
| **AI 추론** | 낮은 지연시간 | NVMe, 프리페치, 캐시 활용 |
| **로그 수집** | 순차 쓰기 | 대용량 블록, EC 풀 |

### 9.2 AI 워크로드 성능 튜닝

```yaml
# AI 워크로드 Ceph 튜닝
ai_ceph_tuning:
  # OSD 튜닝 (AI 스토리지용)
  osd:
    osd_memory_target: 8GB
    osd_op_threads: 16
    osd_disk_threads: 8
    bluestore_cache_size: 4GB

  # 클라이언트 튜닝 (GPU 호스트)
  client:
    rbd_cache: "true"
    rbd_cache_size: 512MB
    rbd_cache_max_dirty: 384MB
    rbd_readahead_max_bytes: 4MB
    rbd_readahead_trigger_requests: 4

  # CephFS 튜닝 (데이터셋용)
  cephfs:
    client_cache_size: 1GB
    client_readahead_min: 1MB
    client_readahead_max: 4MB
```

### 9.3 성능 벤치마크 기준

```
┌─────────────────────────────────────────────────────────────────┐
│                     성능 기준 (Baseline)                         │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  워크로드             4K Random    128K Sequential   지연시간   │
│  ───────────────────────────────────────────────────────────── │
│  Hot Tier (NVMe)      50,000 IOPS  2 GB/s           < 0.5ms    │
│  Warm Tier (SSD)      10,000 IOPS  500 MB/s         < 2ms      │
│  Cold Tier (HDD+EC)   1,000 IOPS   200 MB/s         < 10ms     │
│                                                                 │
│  AI 학습 (순차 읽기)  N/A          3 GB/s           < 1ms      │
│  AI 추론 (랜덤 읽기)  100,000 IOPS N/A              < 0.5ms    │
│                                                                 │
│  * fio 도구로 측정                                              │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 9.4 모니터링 메트릭

```yaml
# 스토리지 모니터링 (Prometheus)
storage_metrics:
  # 용량 메트릭
  capacity:
    - ceph_cluster_total_bytes
    - ceph_cluster_used_bytes
    - ceph_pool_bytes_used{pool="ai-models"}
    - ceph_pool_bytes_used{pool="ai-datasets"}

  # 성능 메트릭
  performance:
    - ceph_osd_op_r_latency
    - ceph_osd_op_w_latency
    - ceph_pool_rd_bytes
    - ceph_pool_wr_bytes

  # AI 스토리지 메트릭
  ai_storage:
    - ai_model_registry_size_bytes
    - ai_dataset_access_count
    - ai_checkpoint_write_bytes

# 알림 규칙
alerts:
  - name: AIStorageCapacityWarning
    condition: ceph_pool_bytes_used{pool="ai-models"} / ceph_pool_max_avail > 0.8
    severity: warning

  - name: AIStorageLatencyHigh
    condition: ceph_osd_op_r_latency{pool=~"ai-.*"} > 5ms
    severity: warning
```

---

## 10. 산출물 및 체크리스트

### 10.1 산출물 목록

| 산출물 | 설명 | 담당 |
|--------|------|------|
| **스토리지 설계서** | 본 문서 | 스토리지 팀 |
| **Ceph 구성 매뉴얼** | 클러스터 구축 절차 | 스토리지 팀 |
| **AI 스토리지 가이드** | AI 워크로드 스토리지 운영 | 스토리지 팀 |
| **백업/복구 절차서** | 백업 및 DR 운영 가이드 | 스토리지 팀 |
| **Terraform 모듈** | storage/ 모듈 | 스토리지 팀 |
| **Ansible Playbook** | 백업/티어링 자동화 | 스토리지 팀 |
| **모니터링 대시보드** | Storage/AI Storage 섹션 | 스토리지 팀 |

### 10.2 설계 검토 체크리스트

- [ ] Ceph 클러스터 구성 완료
- [ ] 테넌트별 풀 생성 및 격리 확인
- [ ] AI 스토리지 풀 구성 (models, datasets, checkpoints, cache)
- [ ] 티어링 정책 적용
- [ ] 스냅샷 정책 설정
- [ ] DR 복제 구성

### 10.3 구현 검증 체크리스트

- [ ] 스토리지 IOPS 벤치마크 통과
- [ ] 테넌트 간 격리 테스트
- [ ] AI 스토리지 접근 제어 테스트
- [ ] 스냅샷 생성/복원 테스트
- [ ] 용량 쿼터 적용 확인
- [ ] DR Failover 테스트
- [ ] AI 워크로드 성능 테스트 (학습/추론)

---

## 부록

### A. 용어 정의

| 용어 | 설명 |
|------|------|
| **OSD** | Object Storage Daemon, Ceph 데이터 저장 데몬 |
| **MON** | Monitor, Ceph 클러스터 상태 관리 |
| **CRUSH** | Controlled Replication Under Scalable Hashing |
| **PG** | Placement Group, 데이터 배치 단위 |
| **RBD** | RADOS Block Device, Ceph 블록 스토리지 |
| **CephFS** | Ceph File System, 분산 파일 시스템 |
| **RGW** | RADOS Gateway, S3 호환 오브젝트 스토리지 |
| **EC** | Erasure Coding, 공간 효율적 데이터 보호 |
| **RPO** | Recovery Point Objective, 데이터 손실 허용 시점 |
| **RTO** | Recovery Time Objective, 복구 목표 시간 |

### B. 참고 자료

- Ceph Documentation: https://docs.ceph.com
- ABLESTACK 스토리지 가이드
- Red Hat Ceph Storage Best Practices
- MinIO Documentation: https://min.io/docs

---

## 관련 문서

- [01_프로젝트_개요서.md](01_프로젝트_개요서.md)
- [02_팀_역할분담.md](02_팀_역할분담.md)
- [03_상세설계_가이드.md](03_상세설계_가이드.md)
- [04_컴퓨트_설계서.md](04_컴퓨트_설계서.md)
- [05_네트워크_보안_설계서.md](05_네트워크_보안_설계서.md)
