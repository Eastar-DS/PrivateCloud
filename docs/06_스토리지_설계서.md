# 스토리지 설계서

## 공공기관 AI 공동 활용 인프라 구축 프로젝트
### AI 기반 문서 태깅 시스템을 위한 멀티 테넌트 프라이빗 클라우드 설계

| 항목 | 내용 |
|------|------|
| 문서명 | 스토리지 설계서 |
| 버전 | 5.0 |
| 작성일 | 2026-02-06 |
| 담당팀 | 스토리지 팀 |

---

**목차**

1. [설계 개요](#1-설계-개요)
2. [분산 스토리지 아키텍처](#2-분산-스토리지-아키텍처)
3. [스토리지 풀 설계](#3-스토리지-풀-설계)
4. [티어링 정책](#4-티어링-정책)
5. [테넌트별 스토리지 격리](#5-테넌트별-스토리지-격리)
6. [문서 태깅 서비스 스토리지](#6-문서-태깅-서비스-스토리지)
7. [스냅샷 및 백업](#7-스냅샷-및-백업)
8. [DR(재해복구) 설계](#8-dr재해복구-설계)
9. [성능 최적화](#9-성능-최적화)
10. [산출물 및 체크리스트](#10-산출물-및-체크리스트)

---

## 1. 설계 개요

### 1.1 목적

본 문서는 문서 태깅 시스템 인프라의 스토리지 아키텍처를 정의한다. Ceph 기반 분산 스토리지를 통해 테넌트(부서) 데이터 격리, 성능 티어링, 데이터 보호를 구현하며, 문서 저장 및 태깅 워크로드를 위한 스토리지 계층을 제공한다.

> **핵심 원칙**: 테넌트(부서)별 문서 저장 공간 격리, 태깅 결과 및 모델 파일 관리

### 1.2 설계 목표

| 목표 | 설명 | 측정 지표 |
|------|------|-----------|
| **데이터 안정성** | 데이터 손실 방지 | 복제 팩터 3, 99.999% 내구성 |
| **테넌트 격리** | 부서 간 데이터/성능 격리 | 풀 분리, IOPS 쿼터 |
| **문서 저장 지원** | 다양한 문서 형식 저장 | PDF, 이미지, Office 등 |
| **비용 효율성** | 데이터 특성별 최적 저장 | Hot/Warm/Cold 티어링 |
| **재해 복구** | 장애 시 신속한 복구 | RPO < 1시간, RTO < 4시간 |
| **확장성** | 용량 증설 용이성 | 무중단 노드 추가 |

### 1.3 적용 범위

```
┌─────────────────────────────────────────────────────────────┐
│                    스토리지 설계 범위                         │
├─────────────────────────────────────────────────────────────┤
│  [In-Scope]                                                 │
│  • Ceph 클러스터 아키텍처                                    │
│  • 스토리지 풀 설계 (블록/파일/오브젝트)                      │
│  • 티어링 정책 수립                                          │
│  • 테넌트(부서)별 쿼터 및 격리                               │
│  • 문서 원본 저장소                                          │
│  • 태깅 결과 저장소                                          │
│  • 모델 파일 저장소                                          │
│  • 스냅샷/백업 정책                                          │
│  • DR 복제 전략                                              │
├─────────────────────────────────────────────────────────────┤
│  [협업 영역]                                                 │
│  • 백엔드망 연계 (네트워크/보안 팀)                          │
│  • VM 스토리지 마운트 (컴퓨트/개발 팀)                       │
├─────────────────────────────────────────────────────────────┤
│  [Out-of-Scope]                                             │
│  • 물리 디스크 구매/설치                                     │
│  • 외부 백업 솔루션 연동                                     │
│  • 외부 백업 연동                                            │
└─────────────────────────────────────────────────────────────┘
```

---

## 2. 분산 스토리지 아키텍처

### 2.1 Ceph 클러스터 개요

ABLESTACK은 Ceph 기반 SDS(Software Defined Storage)를 제공한다. 분산 아키텍처를 통해 단일 장애점 없이 고가용성을 보장한다.

```
┌─────────────────────────────────────────────────────────────────┐
│                     Ceph 클러스터 아키텍처                        │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│    ┌─────────┐    ┌─────────┐    ┌─────────┐                   │
│    │  MON 1  │    │  MON 2  │    │  MON 3  │   ← 모니터 (쿼럼)  │
│    └────┬────┘    └────┬────┘    └────┬────┘                   │
│         │              │              │                         │
│         └──────────────┼──────────────┘                         │
│                        │                                        │
│              ┌─────────┴─────────┐                              │
│              │    CRUSH Map      │   ← 데이터 배치 규칙          │
│              └─────────┬─────────┘                              │
│                        │                                        │
│    ┌───────────────────┼───────────────────┐                   │
│    │                   │                   │                   │
│    ▼                   ▼                   ▼                   │
│ ┌──────┐           ┌──────┐           ┌──────┐                 │
│ │ OSD  │           │ OSD  │           │ OSD  │   ← 스토리지    │
│ │ 1-4  │           │ 5-8  │           │ 9-12 │     데몬        │
│ └──────┘           └──────┘           └──────┘                 │
│  Host 1             Host 2             Host 3                  │
│ (NVMe SSD)        (NVMe SSD)          (NVMe SSD)               │
│                                                                 │
│ ┌──────┐           ┌──────┐           ┌──────┐                 │
│ │ OSD  │           │ OSD  │           │ OSD  │   ← Cold Tier   │
│ │13-16 │           │17-20 │           │21-24 │     (HDD)       │
│ └──────┘           └──────┘           └──────┘                 │
│  Host 4             Host 5             Host 6                  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 2.2 클러스터 구성

| 구성요소 | 수량 | 역할 | 비고 |
|----------|------|------|------|
| **MON** | 3 | 클러스터 상태 관리 | 쿼럼 유지 (과반수) |
| **MGR** | 2 | 관리/모니터링 | Active-Standby |
| **OSD (Hot)** | 12+ | 운영 데이터 저장 | NVMe SSD, 호스트당 4개 |
| **OSD (Cold)** | 12+ | 백업/아카이브 | HDD, 호스트당 4개 |
| **MDS** | 2 | CephFS 메타데이터 | 파일 스토리지용 |
| **RGW** | 2 | S3 호환 오브젝트 스토리지 | 문서 저장용 |

### 2.3 복제 정책

#### 복제 팩터 설정

```yaml
# 풀별 복제 설정
pools:
  # 운영 데이터 - 3중 복제
  production:
    size: 3           # 복제본 수
    min_size: 2       # 최소 복제본 (쓰기 허용 기준)

  # 문서 원본 저장소 - 3중 복제 (데이터 보호)
  documents:
    size: 3
    min_size: 2

  # 백업/아카이브 - Erasure Coding
  archive:
    profile: ec-4-2   # 4 데이터 + 2 패리티
    # 저장 효율: 66% (vs 복제 33%)
```

#### CRUSH 규칙 (장애 도메인)

```
# 데이터 분산 규칙
rule replicated_rule {
    id 0
    type replicated
    min_size 1
    max_size 10
    step take default
    step chooseleaf firstn 0 type host  # 호스트 단위 분산
    step emit
}

# 문서 스토리지 전용 규칙 (NVMe SSD 우선)
rule document_storage_rule {
    id 1
    type replicated
    min_size 1
    max_size 10
    step take nvme-class          # NVMe 디바이스 클래스
    step chooseleaf firstn 0 type host
    step emit
}
```

### 2.4 네트워크 구성

```
┌─────────────────────────────────────────────────────────────────┐
│                   스토리지 네트워크 구성                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   ┌─────────────────────────────────────────────────────────┐  │
│   │  스토리지망 (VLAN 40)                                    │  │
│   │  - IP: 10.0.40.0/24                                     │  │
│   │  - MTU: 9000 (Jumbo Frame)                              │  │
│   │  - 용도: 백엔드 서버 → Ceph 클러스터                     │  │
│   │  - 프로토콜: NFS, S3 (MinIO)                            │  │
│   └─────────────────────────────────────────────────────────┘  │
│                                                                 │
│   ┌─────────────────────────────────────────────────────────┐  │
│   │  Ceph 클러스터 네트워크 (Private)                       │  │
│   │  - IP: 10.0.41.0/24 (별도 VLAN)                         │  │
│   │  - MTU: 9000                                             │  │
│   │  - 용도: OSD 간 복제, 복구 트래픽                        │  │
│   └─────────────────────────────────────────────────────────┘  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## 3. 스토리지 풀 설계

### 3.1 풀 구조

```
┌─────────────────────────────────────────────────────────────────┐
│                        스토리지 풀 구조                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │                     공용 풀 (Shared)                      │  │
│  ├──────────────────────────────────────────────────────────┤  │
│  │  • system-images: OS 템플릿, ISO                         │  │
│  │  • shared-db: 메타데이터 DB 스토리지                      │  │
│  │  • model-storage: 태깅 모델 파일 저장                     │  │
│  └──────────────────────────────────────────────────────────┘  │
│                                                                 │
│  ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐  │
│  │ Dept-A Data     │ │ Dept-B Data     │ │ Dept-C Data     │  │
│  │ (부서 A)        │ │ (부서 B)        │ │ (부서 C)        │  │
│  ├─────────────────┤ ├─────────────────┤ ├─────────────────┤  │
│  │ • documents     │ │ • documents     │ │ • documents     │  │
│  │ • tagging-      │ │ • tagging-      │ │ • tagging-      │  │
│  │   results       │ │   results       │ │   results       │  │
│  │ • temp          │ │ • temp          │ │ • temp          │  │
│  └─────────────────┘ └─────────────────┘ └─────────────────┘  │
│                                                                 │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │                  서비스 풀 (Service)                      │  │
│  ├──────────────────────────────────────────────────────────┤  │
│  │  • queue-data: RabbitMQ 영속 데이터                       │  │
│  │  • cache-data: Redis AOF/RDB                              │  │
│  └──────────────────────────────────────────────────────────┘  │
│                                                                 │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │                     백업 풀 (Backup)                      │  │
│  ├──────────────────────────────────────────────────────────┤  │
│  │  • backup-daily: 일일 백업                                │  │
│  │  • backup-weekly: 주간 백업                               │  │
│  │  • archive: 장기 보관 (EC 적용)                           │  │
│  └──────────────────────────────────────────────────────────┘  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 3.2 풀별 상세 설정

| 풀 이름 | 용도 | 복제 | PG 수 | 쿼터 | 티어 |
|---------|------|------|-------|------|------|
| **system-images** | OS 템플릿, ISO | 3 | 64 | 200GB | Hot |
| **shared-db** | 메타데이터 DB | 3 | 128 | 500GB | Hot |
| **model-storage** | 태깅 모델 파일 | 3 | 64 | 100GB | Hot |
| **dept-a-documents** | 부서 A 문서 원본 | 3 | 128 | 100GB | Hot |
| **dept-a-results** | 부서 A 태깅 결과 | 3 | 64 | 50GB | Hot |
| **dept-b-documents** | 부서 B 문서 원본 | 3 | 128 | 100GB | Hot |
| **dept-b-results** | 부서 B 태깅 결과 | 3 | 64 | 50GB | Hot |
| **dept-c-documents** | 부서 C 문서 원본 | 3 | 128 | 100GB | Hot |
| **dept-c-results** | 부서 C 태깅 결과 | 3 | 64 | 50GB | Hot |
| ~~**search-index**~~ | ~~Elasticsearch~~ | - | - | - | - (제거됨, PostgreSQL 기본 검색 사용) |
| **backup-pool** | 백업 데이터 | EC 4+2 | 256 | 2TB | Cold |

### 3.3 스토리지 유형

#### 3.3.1 블록 스토리지 (RBD)

VM 디스크, 데이터베이스 볼륨에 사용

```yaml
# RBD 이미지 생성 예시
rbd:
  pool: shared-db
  image: postgres-data-01
  size: 100GB
  features:
    - layering      # 스냅샷 지원
    - exclusive-lock
    - object-map    # 성능 최적화
    - fast-diff     # 스냅샷 비교
```

#### 3.3.2 파일 스토리지 (CephFS)

공유 파일 시스템, 문서 저장

```yaml
# CephFS 설정
cephfs:
  name: document-fs
  metadata_pool: cephfs-metadata
  data_pool: cephfs-data

  # 디렉토리별 쿼터
  quotas:
    /dept-a: 100GB
    /dept-b: 100GB
    /dept-c: 100GB
    /models: 100GB
```

#### 3.3.3 오브젝트 스토리지 (RGW)

문서 파일, 태깅 결과, 모델 저장

```yaml
# Rados Gateway 설정 (S3 호환)
rgw:
  zone: default

  # 문서 태깅 서비스 버킷
  buckets:
    - name: documents-dept-a
      quota: 100GB
      versioning: enabled

    - name: documents-dept-b
      quota: 100GB
      versioning: enabled

    - name: documents-dept-c
      quota: 100GB
      versioning: enabled

    - name: tagging-models
      quota: 100GB
      versioning: enabled

    - name: tagging-results
      quota: 200GB
      lifecycle:
        - transition_days: 90
          storage_class: WARM
```

---

## 4. 티어링 정책

### 4.1 스토리지 티어 정의

```
┌─────────────────────────────────────────────────────────────────┐
│                      스토리지 티어 계층                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  HOT TIER (NVMe SSD)                                    │   │
│  │  ─────────────────────────────────────────────────────  │   │
│  │  • 용도: 운영 VM, DB, 최근 문서, 모델 서빙              │   │
│  │  • 성능: IOPS 50,000+, 지연시간 < 0.5ms                 │   │
│  │  • 비용: $$$ (고비용)                                    │   │
│  │  • 대상: 실시간 접근 필요 데이터                         │   │
│  └─────────────────────────────────────────────────────────┘   │
│                           │                                     │
│                           ▼ 30일 미접근                         │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  WARM TIER (SATA SSD)                                   │   │
│  │  ─────────────────────────────────────────────────────  │   │
│  │  • 용도: 최근 백업, 오래된 문서, 이전 태깅 결과         │   │
│  │  • 성능: IOPS 10,000+, 지연시간 < 2ms                   │   │
│  │  • 비용: $$ (중간)                                       │   │
│  │  • 대상: 30일 ~ 90일 보관                                │   │
│  └─────────────────────────────────────────────────────────┘   │
│                           │                                     │
│                           ▼ 90일 미접근                         │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  COLD TIER (HDD + EC)                                   │   │
│  │  ─────────────────────────────────────────────────────  │   │
│  │  • 용도: 장기 아카이브, 규정 준수 데이터                 │   │
│  │  • 성능: IOPS 1,000+, 지연시간 < 10ms                   │   │
│  │  • 비용: $ (저비용)                                      │   │
│  │  • 대상: 90일 이상 보관                                  │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 4.2 티어링 규칙

| 조건 | 동작 | 대상 |
|------|------|------|
| 30일 미접근 | Hot → Warm 이동 | 문서 파일, 태깅 결과 |
| 90일 미접근 | Warm → Cold 이동 | 아카이브 대상 |
| 빈번한 접근 감지 | Cold/Warm → Hot 승격 | 재활성화 데이터 |
| 1년 경과 | 삭제 또는 외부 이전 | 만료 데이터 |

### 4.3 자동 티어링 설정

```yaml
# 티어링 정책 설정
tiering_policy:
  name: auto-tier
  rules:
    - name: hot-to-warm
      condition:
        last_access_days: 30
        min_size: 1MB
      action:
        move_to: warm-pool

    - name: warm-to-cold
      condition:
        last_access_days: 90
      action:
        move_to: cold-pool
        compression: zstd  # 압축 적용

    - name: promote-hot
      condition:
        access_count_per_day: 10
      action:
        move_to: hot-pool
        priority: high
```

---

## 5. 테넌트별 스토리지 격리

### 5.1 격리 계층

```
┌─────────────────────────────────────────────────────────────────┐
│                   테넌트 스토리지 격리 모델                       │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Layer 1: 풀(Pool) 분리                                         │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  dept-a-pool      dept-b-pool      dept-c-pool          │   │
│  │  (완전 분리된 데이터 저장 영역)                          │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
│  Layer 2: 네임스페이스 분리 (CephFS)                            │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  /dept-a/*        /dept-b/*        /dept-c/*            │   │
│  │  (디렉토리 수준 접근 제어)                               │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
│  Layer 3: 접근 키 분리                                          │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  ceph-key-a       ceph-key-b       ceph-key-c           │   │
│  │  (테넌트별 인증 키)                                      │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
│  Layer 4: 네트워크 분리 (VLAN)                                  │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  스토리지망 (VLAN 40)                                    │   │
│  │  + 테넌트별 스토리지 접근 ACL                            │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 5.2 테넌트(부서)별 쿼터 설정

```yaml
# 테넌트(부서) 스토리지 쿼터
tenant_quotas:
  dept-a (부서 A):
    # 문서 저장 쿼터
    documents:
      storage_limit: 100GB
      file_count_limit: 100000
    # 태깅 결과 저장
    tagging_results:
      storage_limit: 50GB
      retention: 1y
    # 임시 저장 영역
    temp:
      storage_limit: 10GB
      retention: 7d

  dept-b (부서 B):
    documents:
      storage_limit: 100GB
      file_count_limit: 100000
    tagging_results:
      storage_limit: 50GB
      retention: 1y
    temp:
      storage_limit: 10GB
      retention: 7d

  dept-c (부서 C):
    documents:
      storage_limit: 100GB
      file_count_limit: 100000
    tagging_results:
      storage_limit: 50GB
      retention: 1y
    temp:
      storage_limit: 10GB
      retention: 7d
```

### 5.3 Ceph 인증 설정

```bash
# 부서별 Ceph 사용자 생성

# 부서 A - 문서 영역
ceph auth get-or-create client.dept-a \
  mon 'allow r' \
  osd 'allow rwx pool=dept-a-documents, allow rwx pool=dept-a-results' \
  mds 'allow rw path=/dept-a' \
  -o /etc/ceph/ceph.client.dept-a.keyring

# 부서 B - 문서 영역
ceph auth get-or-create client.dept-b \
  mon 'allow r' \
  osd 'allow rwx pool=dept-b-documents, allow rwx pool=dept-b-results' \
  mds 'allow rw path=/dept-b' \
  -o /etc/ceph/ceph.client.dept-b.keyring

# 부서 C - 문서 영역
ceph auth get-or-create client.dept-c \
  mon 'allow r' \
  osd 'allow rwx pool=dept-c-documents, allow rwx pool=dept-c-results' \
  mds 'allow rw path=/dept-c' \
  -o /etc/ceph/ceph.client.dept-c.keyring

# 태깅 서비스 계정 (모든 부서 문서 읽기 + 결과 쓰기)
ceph auth get-or-create client.tagging-service \
  mon 'allow r' \
  osd 'allow r pool=dept-a-documents, allow r pool=dept-b-documents, allow r pool=dept-c-documents, allow rwx pool=model-storage' \
  mds 'allow r path=/dept-a, allow r path=/dept-b, allow r path=/dept-c, allow rw path=/models' \
  -o /etc/ceph/ceph.client.tagging-service.keyring
```

---

## 6. 문서 태깅 서비스 스토리지

### 6.1 문서 태깅 스토리지 아키텍처

```
┌─────────────────────────────────────────────────────────────────┐
│                  문서 태깅 스토리지 아키텍처                       │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   ┌─────────────────────────────────────────────────────────┐  │
│   │  백엔드망 (VLAN 30)                                     │  │
│   │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐     │  │
│   │  │ API 서버    │  │ 태깅 워커   │  │ 모델 서버   │     │  │
│   │  │             │  │             │  │             │     │  │
│   │  └──────┬──────┘  └──────┬──────┘  └──────┬──────┘     │  │
│   │         │                │                │             │  │
│   └─────────┼────────────────┼────────────────┼─────────────┘  │
│             │                │                │                 │
│             └────────────────┼────────────────┘                 │
│                              │                                  │
│                              │ 10Gbps (Jumbo Frame)             │
│                              ▼                                  │
│   ┌─────────────────────────────────────────────────────────┐  │
│   │  스토리지망 (VLAN 40)                                    │  │
│   │                                                         │  │
│   │   ┌───────────────┐   ┌───────────────┐                │  │
│   │   │ 문서 원본     │   │  태깅 결과    │                │  │
│   │   │ (S3/CephFS)   │   │  저장소       │                │  │
│   │   │               │   │  (S3/CephFS)  │                │  │
│   │   │ • PDF/이미지  │   │ • 태그 메타   │                │  │
│   │   │ • Office 문서 │   │ • 분류 결과   │                │  │
│   │   │ • 텍스트 파일 │   │ • 신뢰도 점수 │                │  │
│   │   └───────────────┘   └───────────────┘                │  │
│   │                                                         │  │
│   │   ┌───────────────┐   ┌───────────────┐                │  │
│   │   │ 모델 저장소   │   │  임시 저장소  │                │  │
│   │   │               │   │               │                │  │
│   │   │ • 분류 모델   │   │ • 업로드 버퍼 │                │  │
│   │   │ • 임베딩 모델 │   │ • 처리 중간물 │                │  │
│   │   │ • 모델 설정   │   │ • 캐시        │                │  │
│   │   └───────────────┘   └───────────────┘                │  │
│   │                                                         │  │
│   └─────────────────────────────────────────────────────────┘  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 6.2 문서 저장소 설계

#### 6.2.1 문서 원본 저장소

```yaml
# 문서 원본 저장소 설계
document_storage:
  backend: MinIO (S3 호환) + CephFS
  pools:
    - dept-a-documents
    - dept-b-documents
    - dept-c-documents

  # 지원 문서 형식
  supported_formats:
    documents:
      - pdf
      - docx, doc
      - xlsx, xls
      - pptx, ppt
      - txt, rtf
    images:
      - jpg, jpeg, png
      - tiff, bmp
      - gif
    archives:
      - zip (자동 압축 해제)

  # 디렉토리 구조
  directory_structure: |
    documents/
    ├── dept-a/
    │   ├── uploads/           # 업로드 대기
    │   │   └── {date}/{file_id}/
    │   ├── processed/         # 처리 완료
    │   │   └── {date}/{file_id}/
    │   └── failed/            # 처리 실패
    │       └── {date}/{file_id}/
    ├── dept-b/
    │   └── ...
    └── dept-c/
        └── ...

  # 파일 메타데이터
  metadata:
    - file_id (UUID)
    - original_name
    - department_id
    - uploaded_by
    - uploaded_at
    - file_size
    - mime_type
    - checksum (SHA-256)
    - processing_status

  # 용량 계획
  capacity:
    per_department: 100GB
    total: 300GB
    file_size_limit: 100MB  # 단일 파일 최대 크기
```

#### 6.2.2 태깅 결과 저장소

```yaml
# 태깅 결과 저장소 설계
tagging_result_storage:
  backend: PostgreSQL (메타) + S3 (상세)

  # 결과 저장 형식
  result_format:
    metadata_db:
      - document_id
      - department_id
      - tags (array)
      - confidence_scores
      - processed_at
      - model_version

    detail_storage:
      format: json
      location: s3://tagging-results/{dept}/{doc_id}/result.json
      content:
        - raw_predictions
        - embeddings (optional)
        - processing_log

  # 디렉토리 구조
  directory_structure: |
    tagging-results/
    ├── dept-a/
    │   └── {doc_id}/
    │       ├── result.json      # 태깅 결과
    │       ├── preview.json     # 미리보기 데이터
    │       └── metadata.json    # 처리 메타데이터
    ├── dept-b/
    │   └── ...
    └── dept-c/
        └── ...

  # 보관 정책
  retention:
    hot_tier: 90d     # 최근 90일
    warm_tier: 1y     # 1년
    archive: 3y       # 3년 (규정 준수)

  # 용량 계획
  capacity:
    per_department: 50GB
    total: 200GB
```

#### 6.2.3 모델 저장소

```yaml
# 모델 저장소 설계
model_storage:
  backend: S3 (MinIO)
  pool: model-storage

  # 모델 유형
  model_types:
    classification:
      # 문서 분류 모델
      format: pytorch (.pt) or onnx
      typical_size: 200MB - 2GB

    embedding:
      # 텍스트 임베딩 모델
      format: sentence-transformers
      typical_size: 400MB - 1GB

  # 디렉토리 구조
  directory_structure: |
    models/
    ├── classification/
    │   ├── document-tagger/
    │   │   ├── v1.0.0/
    │   │   │   ├── model.pt
    │   │   │   ├── config.json
    │   │   │   └── metadata.json
    │   │   └── v1.1.0/
    │   │       └── ...
    │   └── ...
    └── embedding/
        └── text-encoder/
            └── ...

  # 버전 관리
  versioning:
    enabled: true
    keep_versions: 5
    deployment_marker: deployed.json

  # 용량 계획
  capacity:
    total: 100GB
    model_size_limit: 10GB
```

### 6.3 문서 처리 흐름별 스토리지

```yaml
# 문서 처리 흐름
document_processing_flow:
  # 1. 업로드 단계
  upload:
    source: API Server
    destination: s3://documents-{dept}/uploads/{date}/{file_id}/
    actions:
      - validate_file_type
      - calculate_checksum
      - store_metadata

  # 2. 처리 대기 단계
  pending:
    source: uploads/
    destination: processing_queue (RabbitMQ)
    actions:
      - create_job_record
      - notify_worker

  # 3. 태깅 처리 단계
  processing:
    source: s3://documents-{dept}/uploads/
    temp_location: /tmp/tagging/{job_id}/
    actions:
      - extract_text
      - apply_model
      - generate_tags

  # 4. 결과 저장 단계
  complete:
    document_dest: s3://documents-{dept}/processed/{date}/{file_id}/
    result_dest: s3://tagging-results/{dept}/{doc_id}/
    actions:
      - move_document
      - store_result
      - update_search_index
      - cleanup_temp

  # 5. 실패 처리
  failure:
    destination: s3://documents-{dept}/failed/{date}/{file_id}/
    actions:
      - store_error_log
      - notify_admin
```

### 6.4 스토리지 접근 제어

```yaml
# 문서 태깅 스토리지 RBAC
document_storage_rbac:
  roles:
    # 플랫폼 관리자
    platform-admin:
      permissions:
        - all-documents: rwxd
        - tagging-results: rwxd
        - model-storage: rwxd

    # 부서 관리자
    dept-admin:
      permissions:
        - documents-{own_dept}: rwx
        - tagging-results-{own_dept}: rx
        - model-storage: r

    # 일반 사용자
    dept-user:
      permissions:
        - documents-{own_dept}: rw  # 업로드/조회
        - tagging-results-{own_dept}: r  # 결과 조회

    # 태깅 서비스 (시스템)
    tagging-service:
      permissions:
        - all-documents: r      # 읽기만
        - tagging-results: rw   # 결과 쓰기
        - model-storage: r      # 모델 로드

    # 뷰어
    viewer:
      permissions:
        - documents-{own_dept}: r
        - tagging-results-{own_dept}: r
```

### 6.5 테넌트(부서)별 스토리지 쿼터

| 부서 | 문서 저장 | 태깅 결과 | 임시 영역 | 총 할당 |
|------|----------|----------|----------|--------|
| 부서 A | 100GB | 50GB | 10GB | 160GB |
| 부서 B | 100GB | 50GB | 10GB | 160GB |
| 부서 C | 100GB | 50GB | 10GB | 160GB |
| **공용** | - | - | 50GB | 50GB |
| **모델** | - | - | - | 100GB |

---

## 7. 스냅샷 및 백업

### 7.1 스냅샷 정책

```
┌─────────────────────────────────────────────────────────────────┐
│                      스냅샷 정책 계층                            │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  실시간 스냅샷 (Application-Consistent)                  │   │
│  │  ─────────────────────────────────────────────────────  │   │
│  │  • 트리거: 주요 작업 전 (업그레이드, 패치, 모델 교체)    │   │
│  │  • 보관: 작업 완료 후 24시간                             │   │
│  │  • 용도: 즉시 롤백                                       │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  일일 스냅샷 (Daily)                                     │   │
│  │  ─────────────────────────────────────────────────────  │   │
│  │  • 시간: 매일 02:00 (업무 외 시간)                       │   │
│  │  • 보관: 7일                                             │   │
│  │  • 대상: 모든 VM, DB, 문서 저장소                        │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  주간 스냅샷 (Weekly)                                    │   │
│  │  ─────────────────────────────────────────────────────  │   │
│  │  • 시간: 매주 일요일 03:00                               │   │
│  │  • 보관: 4주                                             │   │
│  │  • 대상: 중요 데이터, 모델 저장소                        │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  월간 스냅샷 (Monthly)                                   │   │
│  │  ─────────────────────────────────────────────────────  │   │
│  │  • 시간: 매월 1일 04:00                                  │   │
│  │  • 보관: 12개월                                          │   │
│  │  • 대상: 규정 준수 대상 데이터                           │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 7.2 모델 버전 관리

```yaml
# 모델 버전 관리 정책
model_versioning:
  # 자동 버전 관리
  auto_versioning:
    trigger:
      - model_upload
      - model_deploy
    format: "v{major}.{minor}.{patch}"

  # 스냅샷 정책
  snapshot:
    before_deploy: true      # 배포 전 스냅샷
    retention:
      deployed_versions: 3   # 배포 버전 3개 유지
      all_versions: 90d      # 전체 90일 유지

  # 롤백 정책
  rollback:
    enabled: true
    max_rollback_versions: 3
```

### 7.3 백업 전략

| 유형 | 주기 | 대상 | 보관 | 저장 위치 |
|------|------|------|------|-----------|
| **증분 백업** | 매일 | 전체 데이터 | 7일 | backup-pool |
| **전체 백업** | 주간 | 전체 데이터 | 4주 | backup-pool |
| **모델 백업** | 배포시 | 모델 파일 | 1년 | archive-pool |
| **문서 백업** | 주간 | 문서 원본 | 90일 | backup-pool |
| **아카이브** | 월간 | 중요 데이터 | 3년 | archive-pool (EC) |

---

## 8. DR(재해복구) 설계

### 8.1 RPO/RTO 정의

```
┌─────────────────────────────────────────────────────────────────┐
│                    RPO/RTO 목표 정의                             │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌───────────────────────────────────────────────────────────┐ │
│  │  서비스 등급별 RPO/RTO                                     │ │
│  ├───────────────────────────────────────────────────────────┤ │
│  │                                                           │ │
│  │  Tier 1 (Critical)       Tier 2 (Important)   Tier 3      │ │
│  │  ──────────────────      ─────────────────    ──────────  │ │
│  │  RPO: 15분               RPO: 1시간           RPO: 4시간  │ │
│  │  RTO: 1시간              RTO: 4시간           RTO: 24시간 │ │
│  │                                                           │ │
│  │  • 메타데이터 DB         • 문서 원본          • 임시 파일 │ │
│  │  • 모델 저장소           • 태깅 결과          • 캐시      │ │
│  │  • 검색 인덱스           • 로그 데이터        • 처리 중간물│ │
│  │                                                           │ │
│  └───────────────────────────────────────────────────────────┘ │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 8.2 데이터 복제 전략

#### 동기 복제 (Tier 1)

```
┌─────────────────────────────────────────────────────────────────┐
│                    동기 복제 (Synchronous)                       │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   Primary Site                         Secondary Site           │
│   ┌─────────────┐     동기 복제        ┌─────────────┐         │
│   │             │  ═══════════════>    │             │         │
│   │   Ceph      │     (실시간)         │   Ceph      │         │
│   │  Cluster    │  <═══════════════    │  Cluster    │         │
│   │             │      ACK             │             │         │
│   └─────────────┘                      └─────────────┘         │
│                                                                 │
│   • 쓰기 완료 = 양쪽 확인 후                                    │
│   • RPO = 0 (데이터 손실 없음)                                  │
│   • 대상: 메타데이터 DB, 모델 저장소                            │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

#### 비동기 복제 (Tier 2/3)

```
┌─────────────────────────────────────────────────────────────────┐
│                   비동기 복제 (Asynchronous)                     │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   Primary Site                         Secondary Site           │
│   ┌─────────────┐     주기적 복제      ┌─────────────┐         │
│   │             │  ───────────────>    │             │         │
│   │   Ceph      │     (15분/1시간)     │   Ceph      │         │
│   │  Cluster    │                      │  Cluster    │         │
│   │             │                      │             │         │
│   └─────────────┘                      └─────────────┘         │
│                                                                 │
│   • RPO = 복제 주기 (15분~1시간)                                │
│   • 대상: 문서 원본, 태깅 결과                                  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 8.3 RBD Mirroring 설정

```yaml
# DR 복제 설정 (rbd-mirror)
rbd_mirror:
  pools:
    - name: shared-db
      mode: pool      # 전체 풀 복제
      schedule: "5m"  # 5분 주기 (중요 데이터)

    - name: model-storage
      mode: pool
      schedule: "5m"

    - name: dept-a-documents
      mode: image
      schedule: "15m"

    - name: dept-b-documents
      mode: image
      schedule: "15m"

    - name: dept-c-documents
      mode: image
      schedule: "15m"
```

---

## 9. 성능 최적화

### 9.1 워크로드별 최적화

| 워크로드 | 최적화 전략 | 설정 |
|----------|-------------|------|
| **메타데이터 DB** | 낮은 지연시간 | rbd_cache=false, NVMe 풀 |
| **문서 업로드** | 높은 처리량 | 대용량 I/O, 병렬 접근 |
| **태깅 처리** | 순차 읽기 최적화 | 프리페치, 대용량 블록 |
| **검색 인덱스** | 랜덤 읽기 최적화 | NVMe, 캐시 활용 |
| **로그 수집** | 순차 쓰기 | 대용량 블록, EC 풀 |

### 9.2 스토리지 성능 튜닝

```yaml
# 문서 태깅 워크로드 Ceph 튜닝
ceph_tuning:
  # OSD 튜닝
  osd:
    osd_memory_target: 4GB
    osd_op_threads: 8
    osd_disk_threads: 4
    bluestore_cache_size: 2GB

  # 클라이언트 튜닝 (API 서버, 태깅 워커)
  client:
    rbd_cache: "true"
    rbd_cache_size: 256MB
    rbd_cache_max_dirty: 192MB
    rbd_readahead_max_bytes: 2MB

  # CephFS 튜닝 (문서 저장)
  cephfs:
    client_cache_size: 512MB
    client_readahead_min: 512KB
    client_readahead_max: 2MB
```

### 9.3 성능 벤치마크 기준

```
┌─────────────────────────────────────────────────────────────────┐
│                     성능 기준 (Baseline)                         │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  워크로드             4K Random    128K Sequential   지연시간   │
│  ───────────────────────────────────────────────────────────── │
│  Hot Tier (NVMe)      50,000 IOPS  2 GB/s           < 0.5ms    │
│  Warm Tier (SSD)      10,000 IOPS  500 MB/s         < 2ms      │
│  Cold Tier (HDD+EC)   1,000 IOPS   200 MB/s         < 10ms     │
│                                                                 │
│  문서 업로드          N/A          500 MB/s         < 2ms      │
│  태깅 처리 (읽기)     N/A          1 GB/s           < 1ms      │
│                                                                 │
│  * fio 도구로 측정                                              │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 9.4 모니터링 메트릭

```yaml
# 스토리지 모니터링 (ABLESTACK Mold 기본 기능 + Ceph Dashboard)
storage_metrics:
  # 용량 메트릭
  capacity:
    - ceph_cluster_total_bytes
    - ceph_cluster_used_bytes
    - ceph_pool_bytes_used{pool="dept-a-documents"}
    - ceph_pool_bytes_used{pool="dept-b-documents"}
    - ceph_pool_bytes_used{pool="dept-c-documents"}
    - ceph_pool_bytes_used{pool="model-storage"}

  # 성능 메트릭
  performance:
    - ceph_osd_op_r_latency
    - ceph_osd_op_w_latency
    - ceph_pool_rd_bytes
    - ceph_pool_wr_bytes

  # 문서 태깅 스토리지 메트릭
  document_storage:
    - document_upload_count
    - document_storage_size_bytes
    - tagging_result_count

# 알림 규칙
alerts:
  - name: DocumentStorageCapacityWarning
    condition: ceph_pool_bytes_used{pool=~"dept-.*-documents"} / ceph_pool_max_avail > 0.8
    severity: warning

  - name: StorageLatencyHigh
    condition: ceph_osd_op_r_latency > 5ms
    severity: warning
```

---

## 10. 산출물 및 체크리스트

### 10.1 산출물 목록

| 산출물 | 설명 | 담당 |
|--------|------|------|
| **스토리지 설계서** | 본 문서 | 스토리지 팀 |
| **Ceph 구성 매뉴얼** | 클러스터 구축 절차 | 스토리지 팀 |
| **문서 스토리지 가이드** | 문서 저장소 운영 | 스토리지 팀 |
| **백업/복구 절차서** | 백업 및 DR 운영 가이드 | 스토리지 팀 |
| **Terraform 모듈** | storage/ 모듈 | 스토리지 팀 |
| **Ansible Playbook** | 백업/티어링 자동화 | 스토리지 팀 |
| **모니터링 대시보드** | Storage 섹션 | 스토리지 팀 |

### 10.2 설계 검토 체크리스트

- [ ] Ceph 클러스터 구성 완료
- [ ] 테넌트(부서)별 풀 생성 및 격리 확인
- [ ] 문서 저장소 구성 (documents, results, models)
- [ ] 티어링 정책 적용
- [ ] 스냅샷 정책 설정
- [ ] DR 복제 구성

### 10.3 구현 검증 체크리스트

- [ ] 스토리지 IOPS 벤치마크 통과
- [ ] 테넌트(부서) 간 격리 테스트
- [ ] 문서 업로드/다운로드 테스트
- [ ] 스냅샷 생성/복원 테스트
- [ ] 용량 쿼터 적용 확인
- [ ] DR Failover 테스트

---

## 부록

### A. 용어 정의

| 용어 | 설명 |
|------|------|
| **OSD** | Object Storage Daemon, Ceph 데이터 저장 데몬 |
| **MON** | Monitor, Ceph 클러스터 상태 관리 |
| **CRUSH** | Controlled Replication Under Scalable Hashing |
| **PG** | Placement Group, 데이터 배치 단위 |
| **RBD** | RADOS Block Device, Ceph 블록 스토리지 |
| **CephFS** | Ceph File System, 분산 파일 시스템 |
| **RGW** | RADOS Gateway, S3 호환 오브젝트 스토리지 |
| **EC** | Erasure Coding, 공간 효율적 데이터 보호 |
| **RPO** | Recovery Point Objective, 데이터 손실 허용 시점 |
| **RTO** | Recovery Time Objective, 복구 목표 시간 |

### B. 참고 자료

- Ceph Documentation: https://docs.ceph.com
- ABLESTACK 스토리지 가이드
- Red Hat Ceph Storage Best Practices
- MinIO Documentation: https://min.io/docs

---

## 관련 문서

- [00_사이징.md](00_사이징.md) - 사이징 가정서 및 요청서
- [00_프로젝트_계획서.md](00_프로젝트_계획서.md) - 프로젝트 계획서
- [00_요구사항_명세서.md](00_요구사항_명세서.md) - 요구사항 명세서
- [01_프로젝트_개요서.md](01_프로젝트_개요서.md) - 프로젝트 개요서
- [02_팀_역할분담.md](02_팀_역할분담.md) - 팀 역할분담
- [03_상세설계_가이드.md](03_상세설계_가이드.md) - 상세설계 가이드
- [04_컴퓨트_설계서.md](04_컴퓨트_설계서.md) - 컴퓨트 설계서
- [05_네트워크_보안_설계서.md](05_네트워크_보안_설계서.md) - 네트워크/보안 설계서

---

## 변경 이력

| 버전 | 날짜 | 변경 내용 | 작성자 |
|------|------|----------|--------|
| 1.0 | 2026-01-30 | 초안 작성 | - |
| 2.0 | 2026-02-02 | AI 스토리지 아키텍처 추가 | - |
| 3.0 | 2026-02-04 | AI 플랫폼 중심 재구성 | - |
| 4.0 | 2026-02-05 | 문서 태깅 시스템용 전면 재구성: 테넌트→부서 A/B/C, 문서 저장소/태깅 결과/모델 저장소 구조, AI 스토리지 섹션 단순화 | - |
| 5.0 | 2026-02-06 | **사이징 반영**: Alliance Storage 1TB + Confidential Storage 1.5TB (500GB x3), 백업 풀 구조 반영 | - |
